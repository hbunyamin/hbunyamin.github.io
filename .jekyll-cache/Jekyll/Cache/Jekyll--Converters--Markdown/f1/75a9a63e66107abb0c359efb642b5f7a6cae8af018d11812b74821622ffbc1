I"\0<p>The subchapter 2.5 of <a href="http://www.stat.columbia.edu/~gelman/book/BDA3.pdf"><strong>Bayesian Data Analysis Third Edition</strong></a> explains how to estimate a normal mean with known variance; particularly, the subchapter extends the development of a normal model with a single observation into the more realistic situation where <em>a sample of independent and identically distributed observations</em> $y = (y_1, \ldots, y_n)$ are available.</p>

<p><a href="/assets/images/normal-dist.jpg"><img src="/assets/images/normal-dist.jpg" alt="img1" class="img-responsive" /></a><em><center>$\pmb{\text{Figure 1}}$: Example of a normal distribution consisting a horde of rabbits. Image taken from <a href="https://vimeo.com/75089338">Casey Dunn</a>, some rights reserved.</center></em></p>

<p>The <em>posterior</em> density of the normal model consists of a <em>likelihood</em> distribution, $\Pr(y \mid \theta)$, and a <em>prior</em> distribution, $\Pr(\theta)$. Specifically,</p>

\[\begin{align}
	y_i \mid \theta &amp;\sim \text{N}(\theta, \sigma^2) &amp;&amp; \text{A normal with mean = }\theta \text{ and variance = }\sigma^2\text{, for }i=1, \ldots, n \\
	\theta          &amp;\sim \text{N}(\mu_0, \tau_0^2)  &amp;&amp;  \text{A normal with mean = }\mu_0 \text{ and variance = }\tau_0^2.
\end{align}\]

<p>Proceeding formally, the posterior density is</p>

<p>\(\begin{align}
\Pr(\theta \mid y) &amp;\propto \Pr(\theta) \Pr(y \mid \theta) &amp;&amp; \text{Posterior definition} \tag{1}\label{eq:definition}\\
                   &amp;= \Pr(\theta) \prod_{i=1}^{n} \Pr(y_i \mid \theta) &amp;&amp; \text{i.i.d observations} \tag{2}\label{eq:iid} \\
                   &amp;\propto \exp \left( -\frac{1}{2 \tau_0^2} (\theta - \mu_0)^2 \right) \prod_{i=1}^n \exp \left( - \frac{1}{2 \sigma^2} (y_i - \theta)^2 \right) &amp;&amp; \text{Normal distributions} \tag{3}\label{eq:exposition-normal} \\
                   &amp;\propto \exp \left( -\frac{1}{2} \left( \frac{1}{\tau_0^2} (\theta - \mu_0)^2 + \frac{1}{\sigma^2} \sum_{i=1}^{n} (y_i - \theta)^2 \right) \right)  &amp;&amp; \text{Sum all terms} \tag{4}\label{eq:sum-all-terms} \\
                   &amp;= \exp \left( -\frac{1}{2} \left( \frac{1}{\tau_0^2} \theta^2 - \frac{2 \theta \mu_0}{\tau_0^2} + \frac{\mu_0^2}{\tau_0^2} + \frac{1}{\sigma^2} \sum_{i=1}^n (y_i^2 - 2 \theta y_i + \theta^2) \right) \right) &amp;&amp; \text{Expand all squares} \tag{5}\label{eq:expand-all} \\
                   &amp;= \exp \left( -\frac{1}{2} \left( \frac{1}{\tau_0^2} \theta^2 - \frac{2 \theta \mu_0}{\tau_0^2} + \frac{\mu_0^2}{\tau_0^2} + \frac{\sum_{i=1}^n y_i^2}{\sigma^2} - \frac{2 \theta \sum_{i=1}^n y_i}{\sigma^2} + \frac{n \theta^2}{\sigma^2} \right) \right) &amp;&amp; \text{Expand the last term} \tag{6}\label{eq:expand-again} \\
                   &amp;= \exp \left( -\frac{1}{2} \left( \frac{\theta^2}{\tau_0^2} + \frac{n \theta^2}{\sigma^2} - 2 \theta \left( \frac{\mu_0}{\tau_0^2} + \frac{\sum_{i=1}^n y_i}{\sigma^2} \right) + \frac{\mu_0^2}{\tau_0^2} + \frac{\sum_{i=1}^n y_i^2}{\sigma^2} \right) \right) &amp;&amp; \text{Group all }\theta s \text{ &amp; } \theta^2 s \tag{7}\label{eq:collect-all} \\
                   &amp;= \exp \left( -\frac{1}{2} \left( \theta^2 \left( \frac{1}{\tau_0^2} + \frac{n}{\sigma^2} \right) - 2 \theta \left( \frac{\mu_0}{\tau_0^2} + \frac{\sum_{i=1}^n y_i}{\sigma^2} \right) + \frac{\mu_0^2}{\tau_0^2} + \frac{\sum_{i=1}^n y_i^2}{\sigma^2} \right) \times \frac{\frac{1}{\frac{1}{\tau_0^2} + \frac{n}{\sigma^2}}}{\frac{1}{\frac{1}{\tau_0^2} + \frac{n}{\sigma^2}}} \right) &amp;&amp; \text{Use a trick} \tag{8}\label{eq:multiply-by} \\
                   &amp;= \exp \left( - \frac{1}{2} \frac{  \left( \theta^2 - 2 \theta \frac{ \frac{\mu_0}{\tau_0^2} + \frac{\sum y_i}{\sigma^2}}{ \frac{1}{\tau_0^2} + \frac{n}{\sigma^2}} + \frac{\frac{\mu_0^2}{\tau_0^2}}{\frac{1}{\tau_0^2} + \frac{n}{\sigma^2}}  + \frac{\frac{\sum y_i^2}{\sigma^2}}{\frac{1}{\tau_0^2} + \frac{n}{\sigma^2}} \right)  }{\frac{1}{\frac{1}{\tau_0^2} + \frac{n}{\sigma^2}}} \right) \tag{9}\label{eq:atas-bawah} \\
                   &amp;= \exp \left( - \frac{1}{2} \frac{\left( \theta - \frac{\frac{\mu_0}{\tau_0^2} + \frac{\sum y_i}{\sigma^2} }{ \frac{1}{\tau_0^2} + \frac{n}{\sigma^2}  }  \right)^2 + C}{\frac{1}{\frac{1}{\tau_0^2} + \frac{n}{\sigma^2}}}   \right) &amp;&amp; \text{With }C \text{ is a constant} \tag{10}\label{eq:a-constant} \\
                   &amp;\propto \exp \left( -\frac{1}{2} \frac{(\theta - \mu_n)^2}{\tau_n^2} \right) \tag{10}\label{eq:almost} \\
                   &amp;\propto \text{N}(\mu_n, \tau_n^2)  &amp;&amp; \text{A normal distribution}        \tag{11}\label{eq:finally}         
\end{align}\)
with 
\(\begin{align}
	\mu_n = \frac{\frac{\mu_0}{\tau_0^2} + \frac{\sum_{i=1}^n y_i}{\sigma^2}}{\frac{1}{\tau_0^2} + \frac{n}{\sigma^2}}}
\end{align}\)</p>

<p>Concretely, we want to show the derivation $J(\theta)$, the <em>Fisher Information</em>, from</p>

\[\begin{equation}
	J(\theta) = \text{E}\left( \left( \frac{d \log \Pr(y \mid \theta )}{d\theta} \right)^2 \, \middle| \, \theta \right) \tag{1}\label{eq:start-point}
\end{equation}\]

<p>to</p>

\[\begin{equation}
	J(\theta) = - \text{E}\left( \frac{d^2 \log \Pr(y \mid \theta)}{d\theta^2} \, \middle| \, \theta \right). \tag{2}\label{eq:end-point}
\end{equation}\]

<p>The idea of this derivation comes from a <a href="https://web.stanford.edu/class/stats311/Lectures/lec-09.pdf"><strong>lecture note by John Duchi from Stanford Statistics class</strong></a>. The difference between this post and the lecture note is that the lecture note deals with <em>multi-variables</em> which employs second derivatives for multi-values (<a href="https://en.wikipedia.org/wiki/Hessian_matrix"><em>Hessian matrix</em></a>); on the other hand, this post deals with a single variable and employs a second derivative for just one value.</p>

<p>Letâ€™s start with computing 
\(\begin{equation} 
	\text{E} \left( \frac{d \log \Pr(y \mid \theta)}{d\theta} \, \middle| \, \theta \right)
\end{equation}\).</p>

\[\require{cancel} \begin{align}
	\text{E}\left( \frac{d \log \Pr(y \mid \theta)}{d\theta} \, \middle| \, \theta \right) &amp;= \int \frac{d \log \Pr(y \mid \theta)}{d\theta} \Pr(y \mid \theta) d\theta  &amp;&amp; \text{definition of expectation} \tag{3}\label{eq:dlog-1} \\
	&amp;= \int \frac{d \Pr(y \mid \theta)}{d\theta} \frac{1}{\Pr(y \mid \theta)} \; \Pr(y \mid \theta) d\theta  &amp;&amp; \text{derivation of }\frac{d \log \Pr(y \mid \theta)}{d\theta} \tag{4}\label{eq:dlog-2} \\
	&amp;= \int \frac{d \Pr(y \mid \theta)}{d\theta} \frac{1}{\cancel{\Pr(y \mid \theta)}} \; \cancel{\Pr(y \mid \theta)}  d\theta \tag{5}\label{eq:dlog-3}	 \\
	&amp;= \int \frac{d \Pr(y \mid \theta)}{d\theta} d\theta  \tag{6}\label{eq:dlog-4}	 \\
	&amp;= \frac{d}{d\theta} \int \Pr(y \mid \theta) d\theta &amp;&amp; \text{exchange }\frac{d}{d\theta} \text{ and } \int \tag{7}\label{eq:dlog-5} \\
	&amp;= \frac{d}{d\theta} \underbrace{\int \Pr(y \mid \theta) d\theta}_{1} &amp;&amp; \text{property of a pdf}\tag{8}\label{eq:dlog-6} \\
	&amp;= \frac{d}{d\theta} (1) \tag{9}\label{eq:dlog-7} 	\\
	&amp;= 0. \tag{10}\label{eq:dlog-8}
\end{align}\]

<p>Consider Equation \eqref{eq:dlog-5}, we shall utilize this exchangeability between <em>integral</em> and <em>differentiation</em> again later.</p>

<p>Equation \eqref{eq:dlog-2} states that</p>

\[\begin{equation}
	\frac{d \log \Pr(y \mid \theta)}{d\theta} = \underbrace{\frac{1}{\Pr(y \mid \theta)}}_{u}  \underbrace{\frac{d \Pr(y \mid \theta)}{d\theta}}_{v}. \tag{11}\label{eq:first-order-derivation}
\end{equation}\]

<p>Therefore,</p>

\[\begin{align}
	\frac{d^2 \log \Pr(y \mid \theta)}{d\theta^2} &amp;= \underbrace{- \frac{1}{\Pr( y \mid \theta )^2} \frac{d \Pr(y \mid \theta)}{d\theta}}_{u^{\prime}} \underbrace{\frac{d \Pr(y \mid \theta)}{d\theta}}_{v}  + \underbrace{\frac{1}{\Pr(y \mid \theta)}}_{u} \underbrace{\frac{d^2 \Pr(y \mid \theta)}{d\theta^2}}_{v^{\prime}} &amp;&amp;  \text{based on } u^{\prime} v + u v^{\prime} \tag{12}\label{eq:second-order-1} \\
	&amp;= \frac{d^2 \Pr(y \mid \theta)}{d\theta^2} \frac{1}{\Pr(y \mid \theta)} - \left( \frac{d\Pr(y \mid \theta)}{d\theta} \frac{1}{\Pr(y \mid \theta)}  \right) \left( \frac{d\Pr(y \mid \theta)}{d\theta} \frac{1}{\Pr(y \mid \theta)}  \right) &amp;&amp; \text{just rearranging} \tag{13}\label{eq:second-order-2} \\
	&amp;= \frac{d^2 \Pr(y \mid \theta)}{d\theta^2} \frac{1}{\Pr(y \mid \theta)} - \left(  \frac{d \log \Pr( y \mid \theta)}{d\theta} \right) \left( \frac{d \log \Pr( y \mid \theta)}{d\theta} \right) &amp;&amp; \text{based on Equation }\eqref{eq:first-order-derivation} \tag{14}\label{eq:second-order-3} \\	
	&amp;= \frac{d^2 \Pr(y \mid \theta)}{d\theta^2} \frac{1}{\Pr(y \mid \theta)} - \left(  \frac{d \log \Pr( y \mid \theta)}{d\theta} \right)^{2} \tag{15}\label{eq:second-order-4}		
\end{align}\]

<p>From Equation \eqref{eq:second-order-4} we obtain</p>

\[\begin{align}
	\frac{d^2 \log \Pr(y \mid \theta)}{d\theta^2} = \frac{d^2 \Pr(y \mid \theta)}{d\theta^2} \frac{1}{\Pr(y \mid \theta)} - \left(  \frac{d \log \Pr( y \mid \theta)}{d\theta} \right)^{2} &amp;\Longleftrightarrow \left(  \frac{d \log \Pr( y \mid \theta)}{d\theta} \right)^{2} = - \frac{d^2 \log \Pr(y \mid \theta)}{d\theta^2} + \frac{d^2 \Pr(y \mid \theta)}{d\theta^2} \frac{1}{\Pr(y \mid \theta)} \tag{16}\label{eq:second-order-last}
\end{align}\]

<p>Now we are ready to calculate
\(\begin{equation}
	\text{E}\left( \left( \frac{d \log \Pr(y \mid \theta )}{d\theta} \right)^2 \, \middle| \, \theta \right).
\end{equation}\)</p>

\[\require{cancel} \begin{align}
	\text{E}\left( \left( \frac{d \log \Pr(y \mid \theta )}{d\theta} \right)^2 \, \middle| \, \theta \right) &amp;= \int \left( \frac{d \log \Pr(y \mid \theta )}{d\theta} \right)^2 \Pr(y \mid \theta) d\theta &amp;&amp; \text{by definition} \tag{17}\label{eq:final-showdown-1}\\
	&amp;= \int \left( - \frac{d^2 \log \Pr(y \mid \theta)}{d\theta^2} + \frac{d^2 \Pr(y \mid \theta)}{d\theta^2} \frac{1}{\Pr(y \mid \theta)} \right) \Pr(y \mid \theta) d\theta &amp;&amp; \text{by Equation }\eqref{eq:second-order-last} \tag{18}\label{eq:final-showdown-2}\\
	&amp;= \int \left( - \frac{d^2 \log \Pr(y \mid \theta)}{d\theta^2}  \right) \Pr(y \mid \theta) d\theta + \int \frac{d^2 \Pr(y \mid \theta)}{d\theta^2} \frac{1}{\Pr(y \mid \theta)} \Pr(y \mid \theta) d\theta &amp;&amp; \text{by distributive} \tag{19}\label{eq:final-showdown-3}\\
	&amp;= \int \left( - \frac{d^2 \log \Pr(y \mid \theta)}{d\theta^2}  \right) \Pr(y \mid \theta) d\theta + \int \frac{d^2 \Pr(y \mid \theta)}{d\theta^2} \frac{1}{\cancel{\Pr(y \mid \theta)}} \cancel{\Pr(y \mid \theta)} d\theta  \tag{20}\label{eq:final-showdown-4}\\		
	&amp;= \int \left( - \frac{d^2 \log \Pr(y \mid \theta)}{d\theta^2}  \right) \Pr(y \mid \theta) d\theta + \int \frac{d^2 \Pr(y \mid \theta)}{d\theta^2} d\theta  \tag{21}\label{eq:final-showdown-5}\\
	&amp;= \int \left( - \frac{d^2 \log \Pr(y \mid \theta)}{d\theta^2}  \right) \Pr(y \mid \theta) d\theta + \frac{d^2}{d\theta^2} \left( \int \Pr(y \mid \theta) d\theta \right) &amp;&amp; \text{by exchangeability again}  \tag{22}\label{eq:final-showdown-6}\\	
	&amp;= \int \left( - \frac{d^2 \log \Pr(y \mid \theta)}{d\theta^2}  \right) \Pr(y \mid \theta) d\theta + \underbrace{\frac{d^2}{d\theta^2} \left( \int \Pr(y \mid \theta) d\theta \right)}_{0} &amp;&amp; \text{similar to Equation }\eqref{eq:dlog-8}  \tag{23}\label{eq:final-showdown-7}\\					
	&amp;= \int \left( - \frac{d^2 \log \Pr(y \mid \theta)}{d\theta^2}  \right) \Pr(y \mid \theta) d\theta \tag{24}\label{eq:final-showdown-8}\\
	&amp;= - \int \left( \frac{d^2 \log \Pr(y \mid \theta)}{d\theta^2}  \right) \Pr(y \mid \theta) d\theta \tag{25}\label{eq:final-showdown-9}\\											
	&amp;= - \text{E}\left( \frac{d^2 \log \Pr(y \mid \theta)}{d\theta^2} \, \middle| \, \theta  \right). &amp;&amp; \text{by definition} \tag{26}\label{eq:final-showdown-10}					
\end{align}\]

<p>At last, we have finally shown that</p>

\[\begin{equation}
	J(\theta) = \text{E}\left( \left( \frac{d \log \Pr(y \mid \theta )}{d\theta} \right)^2 \, \middle| \, \theta \right)  = - \text{E}\left( \frac{d^2 \log \Pr(y \mid \theta)}{d\theta^2} \, \middle| \, \theta \right)
\end{equation}\]

<p>as it is explained by Equation (2.20) on page 53 of the <a href="http://www.stat.columbia.edu/~gelman/book/BDA3.pdf"><strong>book</strong></a>.</p>
:ET