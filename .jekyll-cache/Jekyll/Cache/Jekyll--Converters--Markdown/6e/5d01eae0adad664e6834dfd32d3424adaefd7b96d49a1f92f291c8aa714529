I"h<p>A model called <strong>logistic regression</strong> is introduced in <a href="https://www.coursera.org/learn/machine-learning/home/week/3">Week 3</a> of <a href="https://www.coursera.org/learn/machine-learning/home/welcome"><em>Machine Learning</em> course taught by Andrew Ng</a>. This post tries to explain how to obtain <strong>logistic regression</strong> model from <em>linear regression</em> model which has been explained in <a href="https://www.coursera.org/learn/machine-learning/home/week/1">Week 1</a> and <a href="https://www.coursera.org/learn/machine-learning/home/week/2">Week 2</a> of the course. Therefore, I recommend all readers for studying <a href="https://www.coursera.org/learn/machine-learning/home/week/1">Week 1</a> and <a href="https://www.coursera.org/learn/machine-learning/home/week/2">Week 2</a> before reading this post.</p>

<p>Week 1 of the course shows that <em>linear regression</em> model has the capability to solve classification problem with prediction model (<em>hypothesis</em>) as follows:</p>

\[\begin{equation} \bbox[5px,border:2px solid blue] {h_\theta(x) = \theta^T x} \tag{1}\label{eq:linear-regression} \end{equation}\]

<p>with \(\theta = \begin{bmatrix} \theta_0 &amp; \theta_1 &amp; \theta_2 &amp; \cdots &amp; \theta_n \end{bmatrix}^T\) is called parameter model and \(x = \begin{bmatrix} 1 &amp; x_1 &amp; x_2 &amp; \cdots &amp; x_n \end{bmatrix}^T\) is a <em>test example</em> whose \(y\) we would like to predict.</p>

<p>The question arises as follows:</p>

<blockquote>
  <p>How can we derive <strong>logistic regression</strong> model from <em>linear regression</em> model?</p>
</blockquote>

<p>As we already know that $h_\theta(x)$ in <strong>logistic regression</strong> has a range of values between $0$ and $1$; on the other hand, $h_\theta(x)$ in <em>linear regression</em> has a range of values between $-\infty$ and $\infty$. Therefore, $h_\theta(x)$ which belongs to <strong>logistic regression</strong> needs to be converted into $-\infty &lt; h_\theta(x) &lt; \infty$. Moreover, after the conversion is done, the new $h_\theta(x)$ shall be substituted into Equation \eqref{eq:linear-regression}.<br />
<br /></p>

<p>Let us start by converting $h_\theta(x)$ which belongs to <strong>logistic regression</strong> and then substitute the new $h_\theta(x)$ into Equation \eqref{eq:linear-regression}.</p>

\[\begin{align}  0 &lt; h_\theta(x) &lt; 1 &amp;\Longleftrightarrow  0 &lt; \frac{h_\theta(x)}{1-h_\theta(x)} &lt;  \infty \tag{2}\label{eq:odds-ratio} \\ 
	                    &amp;\Longleftrightarrow  -\infty &lt; \underbrace{\log \left( \frac{h_\theta(x)}{1-h_\theta(x)} \right)}_{\text{the new }h_\theta(x)} &lt; \infty \tag{3}\label{eq:logit}.  \end{align}\]

<p>$\frac{h_\theta(x)}{1-h_\theta(x)}$ in Equation \eqref{eq:odds-ratio} is called <strong>odds ratio</strong> and $\log \left( \frac{h_\theta(x)}{1-h_\theta(x)} \right)$ in Equation \eqref{eq:logit} is named as <strong>logit</strong> function <a href="https://www.packtpub.com/big-data-and-business-intelligence/python-machine-learning-second-edition">(Raschka &amp; Mirjalili, 2017)</a>.</p>

<p>Now that the new $h_\theta(x)$ has the range of values between $-\infty &lt; h_\theta(x) &lt; \infty$, this new $ h_\theta(x) $ can be substituted into Equation \eqref{eq:linear-regression} as follows:</p>

\[\begin{align} \log \left( \frac{h_\theta(x)}{1-h_\theta(x)} \right) = \theta^T x &amp;\Longleftrightarrow e^{\log \left( \frac{h_\theta(x)}{1-h_\theta(x)} \right)} = e^{\theta^T x}   \\
	     &amp;\Longleftrightarrow  \frac{h_\theta(x)}{1-h_\theta(x)} = e^{\theta^T x} \\
	     &amp;\Longleftrightarrow  h_\theta(x) = e^{\theta^T x} - h_\theta(x) e^{\theta^T x}  \\
	     &amp;\Longleftrightarrow  h_\theta(x) + h_\theta(x) e^{\theta^T x} = e^{\theta^T x}   \\ 
	     &amp;\Longleftrightarrow  h_\theta(x) \left( 1 + e^{\theta^T x} \right) = e^{\theta^T x} \\
	     &amp;\Longleftrightarrow  h_\theta(x) = \frac{e^{\theta^T x}}{1 + e^{\theta^T x}}  \\
	     &amp;\Longleftrightarrow  h_\theta(x) = \frac{e^{\theta^T x}}{1 + e^{\theta^T x}}  \times \frac{e^{-\theta^T x}}{e^{-\theta^T x}} \\
	     &amp;\Longleftrightarrow h_\theta(x) = \frac{1}{1+e^{-\theta^T x}}.
	\end{align}\]

<p>Accordingly, <strong>logistic regression</strong> model that we have derived is</p>

\[\begin{equation} \bbox[5px,border:2px solid blue] {h_\theta(x) = \frac{1}{1+e^{-\theta^T x}}} \tag{4}\label{eq:sigmoid} \end{equation}\]

<p><strong>Logistic regression</strong> model in Equation \eqref{eq:sigmoid} is popularly also called <strong>sigmoid function</strong>. Finally, we have successfully derived <strong>logistic regression</strong> model (Equation \eqref{eq:sigmoid}) from <em>linear regression</em> model (Equation \eqref{eq:linear-regression}).   <br />
<br /></p>
<h4 id="references">References</h4>
<p>Raschka, S. and Mirjalili, V. (2017) <em>Python Machine Learning Second Edition</em>. Packt Publishing Ltd. Page 59-60.</p>
:ET