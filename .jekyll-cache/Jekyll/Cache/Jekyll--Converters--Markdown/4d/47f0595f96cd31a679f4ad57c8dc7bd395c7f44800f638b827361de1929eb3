I"‹K<p>This article is inspired by the <em>masterpiece</em> of <a href="https://maranathaedu-my.sharepoint.com/:b:/g/personal/hendra_bunyamin_it_maranatha_edu/EZA9beLgBdZPip9br-UrdSAB024P8IBGESYAbJP3MfRQFQ?e=HdMzZ9"><strong>Gibbs Sampling tutorial</strong></a> by <em>Resnik and Hardisty</em> and also an <em>awesome</em> <a href="https://github.com/bobflagg/gibbs-sampling-for-the-uninitiated"><strong>github repo</strong></a> by <em>Bob Flagg</em>.    Both of these resources are excellent and highly recommended for anyone to read.</p>

<p>This article will show a <em>step by step implementation of a Gibbs sampler</em> for a <strong>Naive Bayes</strong> model in Python.</p>

<p>Letâ€™s start with the problem definition. Assume that we would like to classify whether or not the sentiment of a document is either $0$ (negative) or $1$ (positive) visualized by the following image.</p>

<p><a href="/assets/images/sentiment-positive-negative.jpeg"><img src="/assets/images/sentiment-positive-negative.jpeg" alt="img1" class="img-responsive" /></a><em>An illustration of positive and negative sentiments. Image taken from <a href="https://towardsdatascience.com/cnn-sentiment-analysis-1d16b7c5a0e7">Saad Arshad</a>, some rights reserved.</em></p>

<p>Moreover, the generative story of how the documents are generated as explained in $\S$2.1 of the paper is shown in the following image.</p>

<p><a href="/assets/images/naive-bayes-graphical-model.png"><img src="/assets/images/naive-bayes-graphical-model.png" alt="img1" class="img-responsive" /></a><em>The graphical model of the simple Naive Bayes model.</em></p>

<p>Letâ€™s recall some of the notations from the paper. There are $6$ variables in the image and we shall discuss these variables one by one.</p>

<p>$\pmb{\gamma_\pi}$ is a <em>vectorized version of hyperparameters</em> from a <strong>Beta</strong> distribution. In the literature <a href="https://www.amazon.com/Bayesian-Analysis-Chapman-Statistical-Science/dp/1439840954/ref=sr_1_1?crid=64FB8G3F3C3L&amp;dchild=1&amp;keywords=bayesian+data+analysis&amp;qid=1593868386&amp;s=books&amp;sprefix=bayesian+data%2Caps%2C395&amp;sr=1-1">(Gelman et al., 2013)</a>, these hyperparameters usually are represented by $\alpha$ and $\beta$. In the paper, they are $\gamma_{\pi0}$ and $\gamma_{\pi1}$. Specifically, $\pmb{\gamma_\pi}$ is defined as follows:      <br />
<script type="math/tex">\begin{equation}
    \pmb{\gamma_\pi} = \begin{bmatrix} \gamma_{\pi0} \\ \gamma_{\pi1} \end{bmatrix}.
\end{equation} \tag{1}\label{eq:gamma-pi}</script></p>

<p>Secondly, $\pi$ is a random variable which has a <strong>Beta</strong> distribution, in other words</p>

<script type="math/tex; mode=display">\begin{equation}
    \pi \sim \text{Beta}(\gamma_{\pi0}, \gamma_{\pi1}) \tag{2}\label{eq:beta}. 
\end{equation}</script>

<p>Thirdly, $L_j$ is a random variable for $j$th document which has a <strong>Bernoulli</strong> distribution,</p>

<script type="math/tex; mode=display">\begin{equation}
    L_j \sim \text{Bernoulli}(\pi) \tag{3}\label{eq:binomial}. 
\end{equation}</script>

<p>Fourthly, $\pmb{\gamma_{\theta}}$ is a hyperparameter vector whose dimension is <strong>the size of vocabulary</strong> ($V$) and provided for a <strong>Dirichlet</strong> distribution. In the literature <a href="https://www.amazon.com/Bayesian-Analysis-Chapman-Statistical-Science/dp/1439840954/ref=sr_1_1?crid=64FB8G3F3C3L&amp;dchild=1&amp;keywords=bayesian+data+analysis&amp;qid=1593868386&amp;s=books&amp;sprefix=bayesian+data%2Caps%2C395&amp;sr=1-1">(Gelman et al., 2013)</a>, these hyperparameters usually are represented by $\alpha_1$, $\alpha_2$, $\ldots$, $\alpha_V$. In the paper, it is defined as a vector defined as follows:</p>

<script type="math/tex; mode=display">\begin{equation}
    \pmb{\gamma_{\theta}} = \begin{bmatrix} 
            \gamma_{\theta1} \\
            \gamma_{\theta2} \\
            \vdots \\
            \gamma_{\theta V}
        \end{bmatrix} = \begin{bmatrix} 
            1 \\
            1 \\
            \vdots \\
            1
        \end{bmatrix} \tag{4}\label{eq:gamma-theta}. 
\end{equation}</script>

<p>Fifthly, $\pmb{\theta}$ is a vector which contains two random variables, $\theta_0$ and $\theta_1$. Specifically, both $\theta_0$ and $\theta_1$ are <strong>Dirichlet</strong> distributions with $\pmb{\gamma_{\theta}}$ as their hyperparameters,</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
    \theta_0 &\sim \text{Dirichlet}(\pmb{\gamma_{\theta}}) \tag{5}\label{eq:dirichlet-1}, \\
    \theta_1 &\sim \text{Dirichlet}(\pmb{\gamma_{\theta}}) \tag{6}\label{eq:dirichlet-2}.         
\end{align} %]]></script>

<p>Last but not least, $\pmb{W}_j$ represents a probability distribution of $j$th document which modeled by a <strong>Multinomial</strong> distribution. As the $j$th document has $R_j$ words and probabilities of each word in the vocabulary, the <strong>Multinomial</strong> distribution is stated as follows:</p>

<script type="math/tex; mode=display">\begin{equation}
    \pmb{W}_j \sim \text{Multinomial}(R_j, \theta_{L_j}), \text{ for }j = 1, \ldots, N. \tag{7}\label{eq:multinomial}
\end{equation}</script>

<p>Hopefully, now that we know what those variables are, we can move forward by programming them.</p>

<p>Letâ€™s import all the libraries.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># import all the libraries
</span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">numpy.random</span> <span class="kn">import</span> <span class="n">beta</span><span class="p">,</span> <span class="n">binomial</span><span class="p">,</span> <span class="n">dirichlet</span>
</code></pre></div></div>
<p><br />
Letâ€™s define a function to <em>sample labels</em> for <code class="highlighter-rouge">N</code> documents with hyperparameter $\gamma_{\pi}$ (<code class="highlighter-rouge">gamma_pi</code>). The labels are either <code class="highlighter-rouge">0</code> (<em>negative</em>) or <code class="highlighter-rouge">1</code> (<em>positive</em>); additionally, the number of labels equal to the number of documents.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">sample_labels</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">gamma_pi</span><span class="p">):</span>
    <span class="s">"""Sample labels for N documents according to Beta distribution 
    with a hyperparameter=gamma_pi

    Parameters
    ----------
    N        : int
               Number of documents        
    gamma_pi : list with length=2
               Hyperparameters of the Beta distribution        

    Returns
    -------
    array with shape (N,)
        Labels for the N documents
    """</span>
    <span class="c1"># pi is the Beta distribution with parameters gamma_pi[0] and gamma_pi[1]
</span>    <span class="n">pi</span> <span class="o">=</span> <span class="n">beta</span><span class="p">(</span><span class="n">gamma_pi</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">gamma_pi</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    
    <span class="k">return</span> <span class="n">binomial</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">pi</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span>
</code></pre></div></div>
<p>Therefore, we have defined <script type="math/tex">L_j</script>, for <script type="math/tex">j=1,2, \ldots, N</script> as in the graphical model.</p>

<p>Next, we implement how to generate documents. In generating documents, we employ <em>Poisson distribution</em> to determine the size of a document; in the paper, $R_j$ denotes the size of the $j$th document.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">numpy.random</span> <span class="kn">import</span> <span class="n">multinomial</span><span class="p">,</span> <span class="n">poisson</span>

<span class="k">def</span> <span class="nf">generate_data</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">gamma_pi</span><span class="p">,</span> <span class="n">gamma_theta</span><span class="p">,</span> <span class="n">lmbda</span><span class="p">):</span>
    <span class="s">"""Generate N documents from a hyperparameter of binomial, Dirichlet, and Poisson 

    Parameters
    ----------
    N           : int
                  Number of documents        
    gamma_pi    : list with length=2
                  Hyperparameters of the Beta distribution        
    gamma_theta : list with length=V with V denotes vocabulary size
                  Hyperparameters of the Dirichlet distribution
    lmbda       : int
                  Hyperparameter for Poisson distribution, denotes 
                  number of words in a document
    Returns
    -------
    list of sets with shape (N,)
                  Each set represents a document which contains tuples (i,c)
                  with i denotes word index and c represents frequency of the word index i
    list of integer with shape (N,)
                  Labels of documents  
    """</span>    
    <span class="c1"># Sample N labels for N documents
</span>    <span class="n">labels</span> <span class="o">=</span> <span class="n">sample_labels</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">gamma_pi</span><span class="p">)</span>

    <span class="c1"># Construct two Dirichlet distributions; each has gamma_theta as hyperparameters
</span>    <span class="n">theta</span> <span class="o">=</span> <span class="n">dirichlet</span><span class="p">(</span><span class="n">gamma_theta</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

    <span class="c1"># Initialize a list which contains N documents
</span>    <span class="n">W</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">labels</span><span class="p">:</span>
        <span class="n">R</span> <span class="o">=</span> <span class="n">poisson</span><span class="p">(</span><span class="n">lmbda</span><span class="p">)</span>  <span class="c1"># Sample a size of a document 
</span>        <span class="n">doc</span> <span class="o">=</span> <span class="n">multinomial</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">theta</span><span class="p">[</span><span class="n">l</span><span class="p">])</span> <span class="c1"># Generate a document as a multinomial distribution 
</span>        <span class="c1"># put the document into W
</span>        <span class="c1"># i = word index and c = frequency of the index i  
</span>        <span class="n">W</span><span class="o">.</span><span class="n">append</span><span class="p">({(</span><span class="n">i</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">c</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">doc</span><span class="p">)</span> <span class="k">if</span> <span class="n">c</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">})</span>   
    <span class="k">return</span> <span class="n">W</span><span class="p">,</span> <span class="n">labels</span>
</code></pre></div></div>
<p>Basically, <code class="highlighter-rouge">W</code> contains <code class="highlighter-rouge">N</code> documents and each document consists of tuples and a tuple represents a word index and its frequency. For example, we have <code class="highlighter-rouge">W</code> as follows:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">W</span> <span class="o">=</span> <span class="p">[{(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)},</span>
  <span class="p">{(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">2</span><span class="p">)}]</span>
</code></pre></div></div>
<p>This <code class="highlighter-rouge">W</code> represents $2$ documents. The first document contains word indices <code class="highlighter-rouge">0</code>, <code class="highlighter-rouge">1</code>, <code class="highlighter-rouge">2</code>, <code class="highlighter-rouge">3</code>, <code class="highlighter-rouge">4</code> with their frequencies <code class="highlighter-rouge">1</code>, <code class="highlighter-rouge">2</code>, <code class="highlighter-rouge">2</code>, <code class="highlighter-rouge">3</code>, <code class="highlighter-rouge">2</code> respectively. The second document contains word indices <code class="highlighter-rouge">0</code>, <code class="highlighter-rouge">1</code>, <code class="highlighter-rouge">2</code>, <code class="highlighter-rouge">3</code>, <code class="highlighter-rouge">4</code> with the frequencies <code class="highlighter-rouge">1</code>, <code class="highlighter-rouge">1</code>, <code class="highlighter-rouge">1</code>, <code class="highlighter-rouge">2</code>, <code class="highlighter-rouge">2</code> respectively.</p>

<p>Finally, we define <code class="highlighter-rouge">initialize</code> method as follows:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">initialize</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">gamma_pi</span><span class="p">,</span> <span class="n">gamma_theta</span><span class="p">):</span>
    <span class="s">"""Initialize all random variables and put them all into initial state 

    Parameters
    ----------
    W           : list of sets 
                  List of documents
    labels      : array with shape (N,)
                  Labels for the N documents   
    gamma_pi    : list with length=2
                  Hyperparameters of the Beta distribution        
    gamma_theta : list with length=V with V denotes vocabulary size
                  Hyperparameters of the Dirichlet distribution
    lmbda       : int
                  Hyperparameter for Poisson distribution, denotes 
                  number of words in a document
    Returns
    -------
    A dictionary
                  A dictionary contains
                    C : an array of integers that contains number of negative documents at index 0 and
                                                 number of positive documents at index 1 
                    N : 
    """</span>        
    <span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">W</span><span class="p">)</span>
    <span class="n">M</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
    <span class="n">V</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">gamma_theta</span><span class="p">)</span>

    <span class="n">L</span> <span class="o">=</span> <span class="n">sample_labels</span><span class="p">(</span><span class="n">N</span> <span class="o">-</span> <span class="n">M</span><span class="p">,</span> <span class="n">gamma_pi</span><span class="p">)</span> <span class="c1"># We only sample the unobserved instances
</span>    <span class="n">theta</span> <span class="o">=</span> <span class="n">dirichlet</span><span class="p">(</span><span class="n">gamma_theta</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

    <span class="n">C</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">2</span><span class="p">,))</span>
    <span class="n">C</span> <span class="o">+=</span> <span class="n">gamma_pi</span>
    <span class="n">cnts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="n">V</span><span class="p">))</span>
    <span class="n">cnts</span> <span class="o">+=</span> <span class="n">gamma_theta</span>
    
    <span class="k">for</span> <span class="n">d</span><span class="p">,</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">labels</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span> <span class="o">+</span> <span class="n">L</span><span class="o">.</span><span class="n">tolist</span><span class="p">()):</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">d</span><span class="p">:</span> 
            <span class="n">cnts</span><span class="p">[</span><span class="n">l</span><span class="p">][</span><span class="n">i</span><span class="p">]</span> <span class="o">+=</span> <span class="n">c</span>
        <span class="n">C</span><span class="p">[</span><span class="n">l</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="k">return</span> <span class="p">{</span><span class="s">'C'</span><span class="p">:</span><span class="n">C</span><span class="p">,</span> <span class="s">'N'</span><span class="p">:</span><span class="n">cnts</span><span class="p">,</span> <span class="s">'L'</span><span class="p">:</span><span class="n">L</span><span class="p">,</span> <span class="s">'theta'</span><span class="p">:</span><span class="n">theta</span><span class="p">}</span>    
</code></pre></div></div>
:ET