I"é6<p>This article explains how to create a_one-hot encoding_ of categorical values using PyTorch library. The idea of this post is inspired by ‚Äú<a href="https://www.manning.com/books/deep-learning-with-pytorch?query=deep%20lea"><em>Deep Learning with PyTorch</em></a>‚Äù by Eli Stevens, Luca Antiga, and Thomas Viehmann.</p>

<p>Sooner or later everyone meets categorical values in one‚Äôs dataset. For example, the size of a t-shirt which ranges from <em>small</em> (S), <em>medium</em> (M), <em>large</em> (L), and  <em>extra large</em> (XL), has four categorical values. Next, the question will be</p>

<blockquote>
  <p>How do we encode these categorical values before we feed them into Machine Learning algorithms?</p>
</blockquote>

<p>Suppose that we have installed <a href="https://pytorch.org"><strong>PyTorch</strong></a> in our machine and as an example, we use the <a href="https://maranathaedu-my.sharepoint.com/:x:/g/personal/hendra_bunyamin_it_maranatha_edu/ERfx1C28MeFEuKuNY1ptbKMBEjOFOwxaqfnQIeXYyOF9ww?e=BSnKsb"><em>train set</em></a> of Titanic dataset.</p>

<p><a href="/assets/images/Titanic-Sinking.jpg"><img src="/assets/images/Titanic-Sinking.jpg" alt="img1" class="img-responsive" /></a><em><center>The RMS Titanic sank in the North Atlantic Ocean in the early morning hours of 15 April 1912. Image taken from <a href="https://www.greenscene.co.id/2020/06/25/7-kejadian-nyata-di-titanic-yang-berbeda-dengan-filmnya/">Greenscene</a>, some rights reserved.</center></em></p>

<p>Let‚Äôs get started by reading the train set.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="n">torch</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">edgeitems</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">precision</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">75</span><span class="p">)</span>

<span class="c1"># This is the path to our train set. You might modify it accordingly.
</span><span class="n">TITANIC_DATASET</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span> <span class="s">'/home/hbunyamin/Perkuliahan/Pembelajaran-Mesin-Maranatha/projects/UTS/Titanic'</span> <span class="p">)</span> 
<span class="n">titanic_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span> <span class="n">TITANIC_DATASET</span> <span class="o">/</span> <span class="s">'train.csv'</span> <span class="p">)</span> 
</code></pre></div></div>
<p><br />
Next, we show several rows from the dataframe.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">titanic_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</code></pre></div></div>
<p>Out:
<a href="/assets/images/titanic-head.png"><img src="/assets/images/titanic-head.png" alt="img1" class="img-responsive" /></a>
<br />
We also show the statistics of the Titanic train as follows:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">titanic_df</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</code></pre></div></div>
<p>Out:
<a href="/assets/images/titanic-df-describe.png"><img src="/assets/images/titanic-df-describe.png" alt="img1" class="img-responsive" /></a>
<br />
We are interested in making the passenger classes (<code class="highlighter-rouge">Pclass</code>) column into a one-hot encoding. Let‚Äôs show each value and its frequency inside <code class="highlighter-rouge">Pclass</code> column.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">titanic_df</span><span class="p">[</span><span class="s">'Pclass'</span><span class="p">]</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()</span>
</code></pre></div></div>
<p>Out:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>3    491
1    216
2    184
Name: Pclass, dtype: int64
</code></pre></div></div>
<p>The number of occurrences in the dataset for value <code class="highlighter-rouge">3</code>, <code class="highlighter-rouge">1</code>, and <code class="highlighter-rouge">2</code> are <code class="highlighter-rouge">491</code>, <code class="highlighter-rouge">216</code>, and <code class="highlighter-rouge">184</code> respectively.</p>

<p>Next, we convert <code class="highlighter-rouge">1</code>, <code class="highlighter-rouge">2</code> , and <code class="highlighter-rouge">3</code> into a one-hot encoding. Since indices in PyTorch starts from <code class="highlighter-rouge">0</code>and the values of <code class="highlighter-rouge">Pclass</code> column start from <code class="highlighter-rouge">1</code>, we need to make an adjustment. Let‚Äôs subtract <code class="highlighter-rouge">1</code> from each value in <code class="highlighter-rouge">Pclass</code> column and view the values.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pclass</span> <span class="o">=</span> <span class="n">titanic_df</span><span class="p">[</span><span class="s">'Pclass'</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span>
<span class="n">pclass</span><span class="o">.</span><span class="n">unique</span><span class="p">()</span>
</code></pre></div></div>
<p>Out:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>array([2, 0, 1])
</code></pre></div></div>
<p><br />
Subsequently, we convert the <code class="highlighter-rouge">pclass</code> into a tensor.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pclass</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">pclass</span><span class="p">)</span>
</code></pre></div></div>
<p>Out:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([0, 1, 2])
</code></pre></div></div>
<p><br />
Now we are ready to convert</p>

<script type="math/tex; mode=display">\begin{equation} \texttt{0} \Rightarrow \begin{bmatrix} 1 \\ 
0  \\ 
0 \\ \end{bmatrix}, \; \texttt{1} \Rightarrow \begin{bmatrix} 0 \\ 
1  \\ 
0 \\ \end{bmatrix}, \text{ and } \texttt{2} \Rightarrow \begin{bmatrix} 0 \\ 
0  \\ 
1 \\ \end{bmatrix}. \end{equation}</script>

<p>Firstly, let‚Äôs convert the <code class="highlighter-rouge">pclass</code> into a tensor.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pclass</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">pclass</span><span class="p">)</span>
<span class="n">pclass</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div></div>
<p>Out:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch.Size([891])
</code></pre></div></div>
<p><br />
We initialize the one-hot encoding with a zero matrix with dimension: $891 \times 3$.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pclass_onehot</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">pclass</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">pclass_onehot</span><span class="o">.</span><span class="n">shape</span>
</code></pre></div></div>
<p>Out:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>torch.Size([891, 3])
</code></pre></div></div>
<p><br />
Next, we call <code class="highlighter-rouge">scatter_</code> method. The underscore after the method name means that the method will not return a new tensor; instead, it will modify the tensor in place.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pclass_onehot</span><span class="o">.</span><span class="n">scatter_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">pclass</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="mf">1.0</span><span class="p">)</span>
</code></pre></div></div>
<p>Out:</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tensor([[0., 0., 1.],
        [1., 0., 0.],
        ...,
        [1., 0., 0.],
        [0., 0., 1.]])
</code></pre></div></div>

<p>There are a few optimization algorithms which are faster than <em>gradient descent</em>. In order to understand these optimization algorithms, we need to understand the concept of <strong>exponentially weighted moving averages</strong>. $75^{\circ} \ggg$</p>

<p><a href="/assets/images/temperature-and-days.png"><img src="/assets/images/temperature-and-days.png" alt="img1" class="img-responsive" /></a><em><center>Plots of days . Image taken from <a href="https://towardsdatascience.com/cnn-sentiment-analysis-1d16b7c5a0e7">Saad Arshad</a>, some rights reserved.</center></em></p>

<p>Concretely, the input and hidden layer also have a <em>bias unit</em> respectively (not shown in the image). We stick to Andrew‚Äôs notation from his excellent <a href="https://www.coursera.org/learn/machine-learning/home/welcome">Machine Learning Coursera</a>. Let‚Äôs define our data instance</p>

<script type="math/tex; mode=display">\begin{equation}
		x = \begin{bmatrix}
			1 \\ 
			x_1 \\
			x_2 \\
			x_3
		\end{bmatrix}.\tag{1}\label{eq:data-instance}
	\end{equation}</script>

<p>Our ANN also has the true label, $y$ and two weight matrices, $\Theta^{(1)}$ and $\Theta^{(2)}$ specified as follows:
<script type="math/tex">% <![CDATA[
\begin{align}
	\Theta^{(1)} &= \begin{bmatrix}
		\Theta_{10}^{(1)} & \Theta_{11}^{(1)} & \Theta_{12}^{(1)} & \Theta_{13}^{(1)} \\
		\Theta_{20}^{(1)} & \Theta_{21}^{(1)} & \Theta_{22}^{(1)} & \Theta_{23}^{(1)} \\
		\Theta_{30}^{(1)} & \Theta_{31}^{(1)} & \Theta_{32}^{(1)} & \Theta_{33}^{(1)} 
	\end{bmatrix} \text{ and} \tag{2}\label{eq:weight-matrix-1}   \\
	\Theta^{(2)} &= \begin{bmatrix}
		\Theta_{10}^{(2)} & \Theta_{11}^{(2)} & \Theta_{12}^{(2)} & \Theta_{13}^{(2)} 
	\end{bmatrix}. \tag{3}\label{eq:weight-matrix-2} 		
	\end{align} %]]></script></p>

<p>Now firstly, let us compute for forward propagation. Here is the computation for the hidden layer.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align} 
		\underbrace{z^{(2)}}_{3 \times 1} &= \underbrace{\Theta^{(1)}}_{3 \times 4} \underbrace{x}_{4 \times 1} \tag{4}\label{eq:z-2} \\
		a^{(2)} &= \sigma(z^{(2)})  \tag{5}\label{eq:a-2} \\
		a^{(2)} &= \begin{bmatrix}
			1 \\
			a_1^{(2)} \\
			a_2^{(2)} \\
			a_3^{(2)}
		\end{bmatrix}  & \text{add a bias unit.} \tag{6}\label{eq:a-2-bias}
	\end{align} %]]></script>

<p>Now we compute the output layer as follows:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align} 
		\underbrace{z^{(3)}}_{1 \times 1} &= \underbrace{\Theta^{(2)}}_{1 \times 4} \underbrace{a^{(2)}}_{4 \times 1} \tag{7}\label{eq:z-3} \\
		a^{(3)} &= \sigma(z^{(3)})  \tag{8}\label{eq:a-3} \\
		h_\Theta(x) &= a^{(3)}. \tag{9}\label{eq:h-x}
	\end{align} %]]></script>

<p>We have finished our <strong>forward propagation</strong>. Basically, forward propagation computes our model‚Äôs prediction. Now let us compute the backpropagation steps. Backpropagation algorithm calculates gradients; subsequently, the gradients are needed to update our weight matrices. Consequently, the updates should make our model‚Äôs current prediction better than the previous one.</p>

<p>Here is the backpropagation steps.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align} 
		\underbrace{\delta^{(3)}}_{1 \times 1} &= \underbrace{a^{(3)}}_{1 \times 1} - \underbrace{y}_{1 \times 1} \tag{10}\label{eq:delta-3} \\
		\underbrace{\delta^{(2)}}_{4 \times 1} &= \underbrace{(\Theta^{(2)})^T}_{4 \times 1} \underbrace{\delta^{(3)}}_{1 \times 1} \odot \underbrace{a^{(2)}}_{4 \times 1} \odot \underbrace{(1 - a^{(2)})}_{4 \times 1} & \odot = \text{element-wise multiplication} \tag{11}\label{eq:delta-2} \\
		\delta^{(2)} &= \begin{bmatrix} 
			\delta_1^{(2)} \\
			\delta_2^{(2)} \\
			\delta_3^{(2)}
		\end{bmatrix} & \text{remove }\delta_0^{(2)}. \tag{12}\label{eq:delta-2-remove-bias}
	\end{align} %]]></script>

<p>We have completed our <strong>backpropagation algorithm</strong>.  <br />
Finally we can update our weight matrices as follows:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
		\Theta^{(2)} &= \Theta^{(2)} - \alpha \times \delta^{(3)} (a^{(2)})^T & \alpha = \text{learning rate} \tag{13}\label{eq:update-theta-2} \\
		\Theta^{(1)} &= \Theta^{(1)} - \alpha \times \delta^{(2)} (x)^T. \tag{14}\label{eq:update-theta-1} 		
	\end{align} %]]></script>
:ET