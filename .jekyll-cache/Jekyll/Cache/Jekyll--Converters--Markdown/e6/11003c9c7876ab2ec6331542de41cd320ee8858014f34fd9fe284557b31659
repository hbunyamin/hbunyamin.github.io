I"H<p>Common classification problems in Machine Learning (ML) are binary and multi-class (Sokolova and Lapalme, 2009). For binary classification, we have <em>accuracy</em>, <em>precision</em>, <em>recall</em>, and a combination of <em>precision</em> and <em>recall</em> which is so-called <script type="math/tex">F_1</script> score. However, the <em>precision</em> and <em>recall</em> from binary classification cannot be utilized literally to measure multi-class classification.</p>

<p>To measure the performance of multi-class classification, two important modifications on precision and recall of binary classification are introduced. Their names are <strong>macro-average</strong> and <strong>micro-average</strong>. Therefore, for example, the precision of multi-class classification shall be <strong>macro-average</strong> precision and <strong>micro-average</strong> precision.</p>

<p>Let’s begin with an example of multi-class classification with <script type="math/tex">4</script> classes (<script type="math/tex">0</script>, <script type="math/tex">1</script>, <script type="math/tex">2</script>, and <script type="math/tex">3</script>). Suppose we have <script type="math/tex">\text{our predictions}</script> and the <script type="math/tex">\text{true labels}</script> for five data instances as follows:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align} 
        \text{our predictions} &= [ 0, 0, 2, 2, 3 ],  \\
        \text{true labels} &= [ 0, 1, 3, 3, 3 ] 
    \end{align} %]]></script>

<p>Our first prediction is $0$ and the true label is $0$. Next, our second prediction is $0$ and the true label is $1$. Our third prediction is $2$ while the true label is $3$ and so on. Let’s denote</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align} tp_i &= \text{true positive for class }i \; (i = 0,1,2,3), \\
                fp_i &= \text{false positive for class }i \; (i = 0,1,2,3).  \end{align} %]]></script>

<p>After counting the true and false positives for each class, we obtain</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align} tp_0 &= 1, \; tp_1 = 0, \; tp_2 = 0, \; tp_3 = 1, \text{ and} \\
                fp_0 &= 1, \; fp_1 = 0, \; fp_2 = 2, \; fp_3 = 0.  \end{align} %]]></script>

<p>As we’ve already known, $\text{precision}$ for class $i$ ($\text{precision}_i$) is defined as follows:</p>

<script type="math/tex; mode=display">\begin{equation}
    \text{precision}_i = \frac{tp_i}{tp_i + fp_i}, \text{ for }i = 0,1,2,3. \tag{1}\label{eq:precision-formula}
\end{equation}</script>

<p>Therefore, employing Equation \eqref{eq:precision-formula}, we get</p>

<script type="math/tex; mode=display">\begin{equation}
    \text{precision}_0 = 0.5, \; \text{precision}_1 = 0, \; \text{precision}_2 = 0, \; \text{precision}_3 = 1. \tag{2}\label{eq:precision-results}
\end{equation}</script>

<p>As explained in “<a href="https://datascience.stackexchange.com/questions/15989/micro-average-vs-macro-average-performance-in-a-multiclass-classification-settin">Micro Average vs Macro average Performance in a Multiclass classification setting</a>”, the <strong>macro-average</strong> precision ($\text{precision}_M$) for $4$ classes is defined as</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
    \text{precision}_M &= \frac{\sum_{i=0}^{3}{\text{precision}_i}}{4} \\
                       &= \frac{0.5 + 0 + 0 + 1}{4} \\
                       &= 0.375.
\end{align} %]]></script>

<p>However, the <strong>micro-average</strong> precision ($\text{precision}_\mu$) for $4$ classes is defined as</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
    \text{precision}_\mu &= \frac{\sum_{i=0}^{3}{tp_i}}{\sum_{i=0}^{3}{(tp_i + fp_i)}} \\
                       &= \frac{1+0+0+1}{2+0+2+1} \\
                       &= 0.4.
\end{align} %]]></script>

<p>If there is a <a href="https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/">class imbalance problem</a>, one of the options will be using <strong>weighted macro-average</strong> as performance metrics. The <strong>weighted macro-average</strong> precision ($\text{precision}_{WM}$) for $4$ classes is defined as</p>

<script type="math/tex; mode=display">\begin{equation}
    \text{precision}_{WM} = \sum_{i=0}^{3} \text{weight}_i \times tp_i  \tag{3}\label{eq:weighted-precision}
\end{equation}</script>

<p>with $\text{weight}_i$ is the weight assigned to class $i$ as follows:</p>

<script type="math/tex; mode=display">\begin{equation}
    \text{weight}_i = \frac{tp_i + fp_i}{\sum_{i=0}^{3}{tp_i + fp_i}} \tag{4}\label{eq:weight}
\end{equation}</script>

<p>Using Equation \eqref{eq:weighted-precision} and \eqref{eq:weight}, we compute the <strong>weighted macro-average</strong> precision as follows:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
    \text{precision}_{WM} &= \frac{2}{5} \times 0.5 + 0 \times 0 + \frac{2}{5} \times 0 + \frac{1}{5} \times 1 \\
                          &= 0.4.
\end{align} %]]></script>

<p>Next, we shall show that the <strong>weighted macro-average</strong> precision equals to the <strong>micro-average</strong> precision</p>

<p><br /></p>
<h4 id="references">References</h4>
<p>Sokolova, M. and Lapalme, G. (2009). A systematic analysis of performance measures for classification tasks. <em>Information Processing &amp; Management</em>, 45(4):427 - 437.</p>
:ET