I"d<p>This article explains <strong>forward propagation</strong> and <strong>backpropagation</strong> for one data instance in one round.</p>

<p>Suppose that we have an artificial neural network (ANN) for doing <em>binary classification</em> which consists of one hidden layer. Particularly, the hidden layer has 3 nodes or units as shown in the image below:</p>

<p><a href="/assets/images/ann-one-hidden.png"><img src="/assets/images/ann-one-hidden.png" alt="img1" class="img-responsive" /></a></p>

<p>Concretely, the input and hidden layer also have a <em>bias unit</em> respectively (not shown in the image). We stick to Andrew’s notation from his excellent <a href="https://www.coursera.org/learn/machine-learning/home/welcome">Machine Learning Coursera</a>. Let’s define our data instance</p>

<script type="math/tex; mode=display">\begin{equation}
		x = \begin{bmatrix}
			1 \\ 
			x_1 \\
			x_2 \\
			x_3
		\end{bmatrix}.\tag{1}\label{eq:data-instance}
	\end{equation}</script>

<p>Our ANN also has two weight matrices, $\Theta^{(1)}$ and $\Theta^{(2)}$ specified as follows:
<script type="math/tex">% <![CDATA[
\begin{align}
	\Theta^{(1)} &= \begin{bmatrix}
		\Theta_{10}^{(1)} & \Theta_{11}^{(1)} & \Theta_{12}^{(1)} & \Theta_{13}^{(1)} \\
		\Theta_{20}^{(1)} & \Theta_{21}^{(1)} & \Theta_{22}^{(1)} & \Theta_{23}^{(1)} \\
		\Theta_{30}^{(1)} & \Theta_{31}^{(1)} & \Theta_{32}^{(1)} & \Theta_{33}^{(1)} 
	\end{bmatrix},  \tag{2}\label{eq:weight-matrix-1}   \\
	\Theta^{(2)} &= \begin{bmatrix}
		\Theta_{10}^{(2)} & \Theta_{11}^{(2)} & \Theta_{12}^{(2)} & \Theta_{13}^{(2)} 
	\end{bmatrix}		
	\end{align} %]]></script></p>

:ET