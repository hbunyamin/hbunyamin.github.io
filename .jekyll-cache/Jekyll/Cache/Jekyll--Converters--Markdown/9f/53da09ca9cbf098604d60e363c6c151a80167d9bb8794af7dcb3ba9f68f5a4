I"#?<p>This article answers <strong>Exercise 4.14</strong> from the <em>highly recommended</em> <a href="https://www.bayesrulesbook.com/chapter-4.html#practice-balancing-the-data-prior"><strong>Bayes Rules!</strong></a> book.</p>

<p><a href="/assets/images/bechdel-test.png"><img src="/assets/images/bechdel-test.png" alt="img1" class="img-responsive" /></a><em><center>$\pmb{\text{Figure 1}}$: The Bechdel Test. Image taken from <a href="https://commons.wikimedia.org/wiki/File:Bechdel_test.png">Wikipedia</a>.</center></em></p>

<p>In <a href="https://www.bayesrulesbook.com/chapter-4.html#ch4-priors">Chapter 4 of the book</a>, recall that the <strong>Bechdel test</strong> is satisfied by a movie whose conditions are as follows:</p>
<ul>
  <li>the movie has at least two women in it,</li>
  <li>these two women talk to each other, and</li>
  <li>the two women also talk about something other than a man.</li>
</ul>

<p>$\text{Figure 1}$ summarizes the three rules mentioned before.</p>

<p>Suppose that we review a sample of $n$ recent movies and record $Y$, the number of movies that pass the Bechdel test. Considering $Y$ as the number of “successes” in a fixed number of independence trials, $Y$ can be specified as a Binomial model with $\pi$ as its parameter. Moreover, $\pi$ can also be described as Beta distribution with prior hyperparameters $\alpha$ and $\beta$:</p>

\[\begin{align}
    Y \mid \pi &amp;\sim \text{Bin}(n,\pi)  \\
    \pi        &amp;\sim \text{Beta}(\alpha, \beta).
\end{align}\]

<p>Thus, the posterior of Beta-Binomial model of $\pi$ is given by</p>

\[\begin{equation}
    \pi \mid (Y = y) \sim \text{Beta}(\alpha + y, \beta + n - y). \tag{1}\label{eq:the-posterior}
\end{equation}\]

<p><strong>The Question:</strong></p>
<blockquote>
  <p>In the Beta-Binomial setting, show that we can write the posterior mode of $\pi$ as the weighted average of the prior mode and observed sample success rate:
\(\begin{equation}
    \text{Mode}(\pi \mid Y = y) = \frac{\alpha + \beta - 2}{\alpha + \beta + n - 2} \cdot \text{Mode}(\pi) + \frac{n}{\alpha + \beta + n - 2} \cdot \frac{y}{n} \tag{2}\label{eq:the-problem} 
\end{equation}\)</p>
</blockquote>

<p><strong>Answer</strong>:   <br />
Recall that mode of the prior is</p>

\[\begin{equation}
    \text{Mode}(\pi) = \frac{\alpha - 1}{\alpha + \beta - 2} \tag{3}\label{eq:mode-prior} 
\end{equation}\]

<p>and mode of the posterior is</p>

\[\begin{equation}
    \text{Mode}(\pi \mid Y = y) = \frac{\alpha + y - 1}{\alpha + \beta + n -2}. \tag{4}\label{eq:mode-posterior} 
\end{equation}\]

<p>Next, we show that Equation \eqref{eq:mode-posterior} can be written as Equation \eqref{eq:the-problem} as follows:</p>

\[\begin{align}
    \text{Mode}(\pi \mid Y = y) &amp;= \frac{\alpha + y - 1}{\alpha + \beta + n -2} \\
                                &amp;= \frac{\alpha - 1}{\alpha + \beta + n - 2} + \frac{y}{\alpha + \beta + n - 2}  \\
                                &amp;= \frac{\alpha - 1}{\alpha + \beta + n - 2} \cdot \frac{\alpha + \beta -2}{\alpha + \beta -2} + \frac{y}{\alpha + \beta + n - 2} \cdot \frac{n}{n} \\
                                &amp;= &amp;&amp; \text{rearrange the terms}
\end{align}\]

<p>where $\mu$ is a column vector of length $d$ and $\Sigma$ is a known $d \times d$ variance matrix, which is <a href="https://en.wikipedia.org/wiki/Symmetric_matrix"><em>symmetric</em></a> and <a href="https://en.wikipedia.org/wiki/Definite_matrix"><em>positive definite</em></a>. Therefore, the <em>likelihood function</em> for a single observation is</p>

\[\begin{equation}
    \Pr(y \mid \mu, \Sigma) \propto \lvert \Sigma \rvert^{-1/2} \exp \left( - \frac{1}{2} (y-\mu)^T \Sigma^{-1} (y - \mu) \right),  \tag{2}\label{eq:likelihood-one-sample} 
\end{equation}\]

<p>and for a sample of $n$ independent and identically distributed observations, $y_1, \ldots, y_n$, is</p>

\[\begin{align}
   \Pr( y_1, \ldots, y_n \mid \mu, \Sigma ) &amp;\propto \prod_{i=1}^{n}{ \Pr( y_i \mid \mu, \Sigma ) }     \tag{3}\label{eq:likelihood-samples-1}  \\
   &amp;= \prod_{i=1}^{n}{ \lvert \Sigma \rvert^{-1/2} \exp \left( - \frac{1}{2} (y_i-\mu)^T \Sigma^{-1} (y_i - \mu) \right) }     \tag{4}\label{eq:likelihood-samples-2} &amp;&amp; \text{using Equation }\eqref{eq:likelihood-one-sample} \\
   &amp;= \prod_{i=1}^{n}{ \lvert \Sigma \rvert^{-1/2} } \prod_{i=1}^{n}{\exp \left( - \frac{1}{2} (y_i-\mu)^T \Sigma^{-1} (y_i - \mu) \right) }     \tag{5}\label{eq:likelihood-samples-3}  \\  
   &amp;= \lvert \Sigma \rvert^{-n/2} \exp \left( - \frac{1}{2} \sum_{i=1}^{n}{(y_i-\mu)^T \Sigma^{-1} (y_i - \mu)} \right).  \tag{6}\label{eq:likelihood-samples-4}  \\     
\end{align}\]

<p>Actually, given the following nice <a href="https://en.wikipedia.org/wiki/Trace_(linear_algebra)"><em>trace property</em></a>,</p>

\[\begin{equation}
    \sum_{i=1}^{n}{x_i^T A x_i} = \text{tr}\left( A \sum_{i=1}^{n}{x_i x_i^T} \right) \tag{7}\label{eq:trace-property}
\end{equation}\]

<p>with $x_i$ is a column vector whose dimension is $d \times 1$, $A$ is a symmetric matrix whose dimension is $d \times d$, and $\text{tr}$ is a <a href="https://en.wikipedia.org/wiki/Trace_(linear_algebra)"><em>trace function</em></a>, we can rewrite Equation \eqref{eq:trace-property} as follows:</p>

\[\begin{equation}
\Pr( y_1, \ldots, y_n \mid \mu, \Sigma ) \propto \lvert \Sigma \rvert^{-n/2} \exp \left( -\frac{1}{2} \text{tr}(\Sigma^{-1} S_0) \right) \tag{8}\label{eq:likelihood-final-version}
\end{equation}\]

<p>where $S_0$ is the “<em>sums squares</em>” matrix relative to $\mu$,</p>

\[\begin{equation}
    S_0 = \sum_{i=1}^{n}{(y_i - \mu)(y_i - \mu)^T}. \tag{9}\label{eq:sum-of-squares}
\end{equation}\]

<p>Before we construct the posterior distribution of the model, let’s define the <em>prior distribution</em> as follows:</p>

\[\begin{equation}
\Pr( \mu ) \propto  \lvert \Lambda_0 \rvert^{-1/2} \exp \left(-\frac{1}{2} (\mu - \mu_0)^T \Lambda_0^{-1} (\mu - \mu_0)  \right) \tag{10}\label{eq:prior}
\end{equation}\]

<p>that is $\mu \sim \text{N}(\mu_0, \Lambda_0)$. By the way, $\Lambda_0$ is also a symmetric and positive definite matrix as well.</p>

<blockquote>
  <p>Now that we have both <em>likelihood</em> and <em>prior</em> distributions; let’s compute the posterior distribution of the model,</p>
</blockquote>

\[\begin{align}
\Pr( \mu \mid y, \Sigma ) &amp;\propto \Pr( y \mid \mu, \Sigma ) \Pr(\mu \mid \Sigma) &amp;&amp; \text{by Bayes rule} \tag{11}\label{eq:posterior-def}  \\
                          &amp;= \lvert \Sigma \rvert^{-n/2} \exp \left( - \frac{1}{2} \sum_{i=1}^{n}{(y_i-\mu)^T \Sigma^{-1} (y_i - \mu)} \right) \times \lvert \Lambda_0 \rvert^{-1/2} \exp \left(-\frac{1}{2} (\mu - \mu_0)^T \Lambda_0^{-1} (\mu - \mu_0)  \right)  \\
                          &amp;\propto \exp \left( -\frac{1}{2} \underbrace{ \left( (\mu - \mu_0)^T \Lambda_0^{-1} (\mu - \mu_0) + \sum_{i=1}^{n}{(y_i-\mu)^T \Sigma^{-1} (y_i - \mu)} \right)}_{\text{A}}   \right) \tag{12}\label{eq:posterior-1}
\end{align}\]

<blockquote>
  <p>Part $\text{A}$ in Equation \eqref{eq:posterior-1} is actually a “<em>completing the quadratic form</em>” problem.</p>
</blockquote>

<p>Let’s solve the problem as follows:</p>

\[\begin{align}
\text{A} &amp;= (\mu^T - \mu_0^T) \Lambda_0^{-1} (\mu - \mu_0)  + \sum_{i=1}^{n}{(y_i^T - \mu^T)\Sigma^{-1}(y_i - \mu)} &amp;&amp; \text{by transpose property} \tag{13}\label{eq:complete-squares-1}  \\
&amp;= \underbrace{(\mu^T \Lambda_0^{-1} - \mu_0^T \Lambda_0^{-1}) (\mu - \mu_0)}_{\text{B}}  + \underbrace{\sum_{i=1}^{n}{(y_i^T \Sigma^{-1} - \mu^T \Sigma^{-1})(y_i - \mu)}}_{\text{C}} \tag{14}\label{eq:complete-squares-2} 
\end{align}\]

<p>Let’s multiply out all terms in part $\text{B}$ in Equation \eqref{eq:complete-squares-2} as follows:</p>

\[\begin{align}
    \text{B} &amp;= \mu^T \Lambda_0^{-1} \mu - \underbrace{\mu^T \Lambda_0^{-1} \mu_0}_{\text{a scalar}} - \underbrace{\mu_0^T \Lambda_0^{-1} \mu}_{\text{a scalar}} + \mu_0^T \Lambda_0^{-1} \mu_0  \tag{15}\label{eq:b-1} \\
             &amp;= \mu^T \Lambda_0^{-1} \mu - \mu^T \Lambda_0^{-1} \mu_0 - (\mu^T \Lambda_0^{-1} \mu_0)^T + \mu_0^T \Lambda_0^{-1} \mu_0  &amp;&amp; \text{by transpose property} \tag{16}\label{eq:b-2} \\
             &amp;= \mu^T \Lambda_0^{-1} \mu - \mu^T \Lambda_0^{-1} \mu_0 - \mu^T \Lambda_0^{-1} \mu_0 + \mu_0^T \Lambda_0^{-1} \mu_0 &amp;&amp; \text{as } \mu^T \Lambda_0^{-1} \mu_0 = (\mu^T \Lambda_0^{-1} \mu_0)^T  \tag{17}\label{eq:b-3}  \\             
             &amp;= \mu^T \Lambda_0^{-1} \mu - 2 \mu^T \Lambda_0^{-1} \mu_0 + \mu_0^T \Lambda_0^{-1} \mu_0. \tag{18}\label{eq:b-4}  \\             
\end{align}\]

<p>Let’s also multiply out part $\text{C}$ in Equation \eqref{eq:complete-squares-2},</p>

\[\begin{align}
    \text{C} &amp;= \sum_{i=1}^{n}{(y_i^T \Sigma^{-1} y_i - \underbrace{y_i^T \Sigma^{-1} \mu}_{\text{scalar}} - \underbrace{\mu^T \Sigma^{-1} y_i}_{\text{scalar}} + \mu^T \Sigma^{-1} \mu)} \tag{19}\label{eq:c-1} \\
    &amp;= \sum_{i=1}^{n}{(y_i^T \Sigma^{-1} y_i - (\mu^T \Sigma^{-1} y_i)^T - \mu^T \Sigma^{-1} y_i + \mu^T \Sigma^{-1} \mu)} &amp;&amp; \text{by transpose property} \tag{20}\label{eq:c-2} \\    
    &amp;= \sum_{i=1}^{n}{(y_i^T \Sigma^{-1} y_i - \mu^T \Sigma^{-1} y_i - \mu^T \Sigma^{-1} y_i + \mu^T \Sigma^{-1} \mu)} &amp;&amp; \text{as } (\mu^T \Sigma^{-1} y_i)^T = \mu^T \Sigma^{-1} y_i \tag{21}\label{eq:c-3} \\        
    &amp;= \sum_{i=1}^{n}{(y_i^T \Sigma^{-1} y_i - 2 \mu^T \Sigma^{-1} y_i + \mu^T \Sigma^{-1} \mu)}  \tag{22}\label{eq:c-4} \\
    &amp;= \sum_{i=1}^{n}{y_i^T \Sigma^{-1} y_i} - \sum_{i=1}^{n}{2 \mu^T \Sigma^{-1} y_i}  + \sum_{i=1}^{n}{\mu^T \Sigma^{-1} \mu} &amp;&amp; \text{by a linear operator of }\sum \tag{23}\label{eq:c-5} \\
    &amp;= \sum_{i=1}^{n}{y_i^T \Sigma^{-1} y_i} - 2 \mu^T \Sigma^{-1} \sum_{i=1}^{n}{y_i}  + \sum_{i=1}^{n}{\mu^T \Sigma^{-1} \mu} \tag{24}\label{eq:c-6}\\
    &amp;= \sum_{i=1}^{n}{y_i^T \Sigma^{-1} y_i} - 2 \mu^T \Sigma^{-1} n \overline{y}  + \sum_{i=1}^{n}{\mu^T \Sigma^{-1} \mu}  &amp;&amp; \text{as }\overline{y} = \frac{\sum_{i=1}^n y_i}{n} \tag{25}\label{eq:c-7} \\
    &amp;= \sum_{i=1}^{n}{y_i^T \Sigma^{-1} y_i} - 2 \mu^T \Sigma^{-1} n \overline{y}  + n \mu^T \Sigma^{-1} \mu  &amp;&amp; \text{as }\sum_{i=1}^{n}{\text{constant}} = n \times \text{constant} \tag{26}\label{eq:c-8} \\    
    &amp;= \sum_{i=1}^{n}{y_i^T \Sigma^{-1} y_i} - 2 \mu^T n \Sigma^{-1}  \overline{y}  +  \mu^T n \Sigma^{-1} \mu  \tag{27}\label{eq:c-9}      \\
    &amp;= \mu^T n \Sigma^{-1} \mu - 2 \mu^T n \Sigma^{-1}  \overline{y} + \sum_{i=1}^{n}{y_i^T \Sigma^{-1} y_i} &amp;&amp; \text{just rearrange terms}   \tag{28}\label{eq:c-10}          
\end{align}\]

<p>Now let’s combine both part $\text{B}$ (Equation \eqref{eq:b-4}) and part $\text{C}$ (Equation \eqref{eq:c-10}) into part $\text{A}$ in Equation \eqref{eq:complete-squares-2},</p>

\[\begin{align}
\text{A} =&amp; \mu^T \Lambda_0^{-1} \mu - 2 \mu^T \Lambda_0^{-1} \mu_0 + \mu_0^T \Lambda_0^{-1} \mu_0 + \\
          &amp; \mu^T n \Sigma^{-1} \mu - 2 \mu^T n \Sigma^{-1}  \overline{y} + \sum_{i=1}^{n}{y_i^T \Sigma^{-1} y_i} \tag{29}\label{eq:c-11} \\
         =&amp; \mu^T (\Lambda_0^{-1} + n \Sigma^{-1} ) \mu - 2 \mu^T ( \Lambda_0^{-1} \mu_0 + n \Sigma^{-1} \overline{y} ) + \underbrace{\mu_0^T \Lambda_0^{-1} \mu_0 + \sum_{i=1}^{n}{y_i^T \Sigma^{-1} y_i}}_{\text{constant}_1}  &amp;&amp; \text{sum all terms accordingly} \tag{30}\label{eq:c-12} \\
         =&amp; \mu^T (\Lambda_0^{-1} + n \Sigma^{-1} ) \mu - 2 \mu^T ( \Lambda_0^{-1} \mu_0 + n \Sigma^{-1} \overline{y} ) + \text{constant}_1  \tag{31}\label{eq:c-13} \\         
         =&amp; \mu^T (\Lambda_0^{-1} + n \Sigma^{-1} ) \mu - \underbrace{\mu^T ( \Lambda_0^{-1} \mu_0 + n \Sigma^{-1} \overline{y} )}_{\text{scalar}} - \underbrace{\mu^T ( \Lambda_0^{-1} \mu_0 + n \Sigma^{-1} \overline{y} )}_{\text{scalar}} + \text{constant}_1 &amp;&amp; \text{separate the middle term}  \tag{32}\label{eq:c-14} \\         
         =&amp; \mu^T (\Lambda_0^{-1} + n \Sigma^{-1} ) \mu - \mu^T ( \Lambda_0^{-1} \mu_0 + n \Sigma^{-1} \overline{y} ) - ( \Lambda_0^{-1} \mu_0 + n \Sigma^{-1} \overline{y} )^T \mu  + \text{constant}_1 &amp;&amp; \text{by transpose property}  \tag{33}\label{eq:c-15} \\    
         =&amp; \left( \mu^T (\Lambda_0^{-1} + n \Sigma^{-1} ) - (\Lambda_0^{-1} \mu_0 + n \Sigma^{-1} \overline{y})^T \right) \left( \mu - (\Lambda_0^{-1} + n \Sigma^{-1})^{-1} (\Lambda_0^{-1} \mu_0 + n \Sigma^{-1} \overline{y}) \right)  \underbrace{- (\Lambda_0^{-1} \mu_0 + n \Sigma^{-1} \overline{y})^T (\Lambda_0^{-1} + n \Sigma^{-1})^{-1} (\Lambda_0^{-1} \mu_0 + n \Sigma^{-1} \overline{y}) + \text{constant}_1 }_{\text{constant}_2}  &amp;&amp; \text{by factoring &amp; inverse matrix}  \tag{34}\label{eq:c-16} \\             
         =&amp; \left( \mu^T (\Lambda_0^{-1} + n \Sigma^{-1} ) - (\Lambda_0^{-1} \mu_0 + n \Sigma^{-1} \overline{y})^T \right) \left( \mu - (\Lambda_0^{-1} + n \Sigma^{-1})^{-1} (\Lambda_0^{-1} \mu_0 + n \Sigma^{-1} \overline{y}) \right) + \text{constant}_2    \tag{35}\label{eq:c-17} \\                      
         =&amp; \left( \mu^T  - (\Lambda_0^{-1} \mu_0 + n \Sigma^{-1} \overline{y})^T (\Lambda_0^{-1} + n \Sigma^{-1} )^{-1} \right) (\Lambda_0^{-1} + n \Sigma^{-1} ) \left( \mu - (\Lambda_0^{-1} + n \Sigma^{-1})^{-1} (\Lambda_0^{-1} \mu_0 + n \Sigma^{-1} \overline{y}) \right) + \text{constant}_2  &amp;&amp; \text{get }(\Lambda_0^{-1} + n \Sigma^{-1} ) \text{ out}  \tag{36}\label{eq:c-18} \\                      
         =&amp; \left( \mu^T  - (\Lambda_0^{-1} \mu_0 + n \Sigma^{-1} \overline{y})^T (\Lambda_0^{-1} + n \Sigma^{-1} )^{-T} \right) (\Lambda_0^{-1} + n \Sigma^{-1} ) \left( \mu - (\Lambda_0^{-1} + n \Sigma^{-1})^{-1} (\Lambda_0^{-1} \mu_0 + n \Sigma^{-1} \overline{y}) \right) + \text{constant}_2  &amp;&amp; \text{symmetric property, }A^{-T} = A^{-1}  \tag{37}\label{eq:c-19} \\                               
         =&amp; \left( \mu  - (\Lambda_0^{-1} + n \Sigma^{-1} )^{-1} (\Lambda_0^{-1} \mu_0 + n \Sigma^{-1} \overline{y})  \right)^T (\Lambda_0^{-1} + n \Sigma^{-1} ) \left( \mu - (\Lambda_0^{-1} + n \Sigma^{-1})^{-1} (\Lambda_0^{-1} \mu_0 + n \Sigma^{-1} \overline{y}) \right) + \text{constant}_2  &amp;&amp; \text{by transpose property}  \tag{38}\label{eq:c-20} \\        
         =&amp; \left( \mu  - \mu_n  \right)^T \Lambda_n^{-1} \left( \mu - \mu_n \right) + \text{constant}_2    \tag{39}\label{eq:c-21} \\                                  
\end{align}\]

<p>where</p>

\[\begin{align}
\mu_n &amp;= (\Lambda_0^{-1} + n \Sigma^{-1} )^{-1} (\Lambda_0^{-1} \mu_0 + n \Sigma^{-1} \overline{y}) \text{ and} \nonumber \\
\Lambda_n^{-1} &amp;= \Lambda_0^{-1} + n \Sigma^{-1}. \tag{40}\label{eq:final-mean-variance} 
\end{align}\]

<p>Next, let’s substitute the part $\text{A}$ (Equation \eqref{eq:c-21}) into the posterior distribution (Equation \eqref{eq:posterior-1}),</p>

\[\begin{align}
\Pr( \mu \mid y, \Sigma ) &amp;\propto \exp \left( -\frac{1}{2} \underbrace{ \left( (\mu - \mu_0)^T \Lambda_0^{-1} (\mu - \mu_0) + \sum_{i=1}^{n}{(y_i-\mu)^T \Sigma^{-1} (y_i - \mu)} \right)}_{\text{A}}   \right) \tag{41}\label{eq:posterior-2} \\
&amp;= \exp \left( -\frac{1}{2} \left( \left( \mu  - \mu_n  \right)^T \Lambda_n^{-1} \left( \mu - \mu_n \right)  + \text{constant}_2 \right) \right) \tag{42}\label{eq:posterior-3} \\
&amp;= \exp \left( -\frac{1}{2}  \left( \mu  - \mu_n  \right)^T \Lambda_n^{-1} \left( \mu - \mu_n \right)   \right) \times \exp \left( \text{constant}_2 \right) \tag{43}\label{eq:posterior-4} \\
&amp;\propto  \exp \left( -\frac{1}{2}  \left( \mu  - \mu_n  \right)^T \Lambda_n^{-1} \left( \mu - \mu_n \right)   \right) \tag{44}\label{eq:posterior-5} \\
&amp;= \text{N}(\mu \mid \mu_n, \Lambda_n) \tag{45}\label{eq:posterior-6}
\end{align}\]

<p>where</p>

\[\begin{align}
\mu_n &amp;= (\Lambda_0^{-1} + n \Sigma^{-1} )^{-1} (\Lambda_0^{-1} \mu_0 + n \Sigma^{-1} \overline{y})  \nonumber \\
\Lambda_n^{-1} &amp;= \Lambda_0^{-1} + n \Sigma^{-1}. \nonumber
\end{align}\]

<p>By the way, the above derivation is also mentioned as <em>Exercise 3.13</em> in <a href="http://www.stat.columbia.edu/~gelman/book/BDA3.pdf"><em>the book</em></a>.</p>
:ET