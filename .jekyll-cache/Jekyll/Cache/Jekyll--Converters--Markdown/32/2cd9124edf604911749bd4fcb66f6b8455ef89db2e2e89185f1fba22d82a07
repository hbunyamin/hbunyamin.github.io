I"Œ<p>This article explains <strong>forward propagation</strong> and <strong>backpropagation</strong> for one data instance in one round.</p>

<p>Suppose that we have an artificial neural network (ANN) for doing <em>binary classification</em> which consists of one hidden layer. Particularly, the hidden layer has 3 nodes or units as shown in the image below:</p>

<p><a href="/assets/images/ann-one-hidden.png"><img src="/assets/images/ann-one-hidden.png" alt="img1" class="img-responsive" /></a></p>

<p>Concretely, the input and hidden layer also have a <em>bias unit</em> respectively (not shown in the image). We stick to Andrewâ€™s notation from his excellent <a href="https://www.coursera.org/learn/machine-learning/home/welcome">Machine Learning Coursera</a>. Letâ€™s define our data instance</p>

<script type="math/tex; mode=display">\begin{equation}
		x = \begin{bmatrix}
			1 \\ 
			x_1 \\
			x_2 \\
			x_3
		\end{bmatrix}.\tag{1}\label{eq:data-instance}
	\end{equation}</script>

<p>Our ANN also has two weight matrices, $\Theta^{(1)}$ and $\Theta^{(2)}$ specified as follows:
$$
	\begin{align}
	\Theta^{(1)} &amp;= \begin{bmatrix}
		\Theta_{10}^{(1)} &amp; \Theta_{11}^{(1)} &amp; \Theta_{12}^{(1)} &amp; \Theta_{13}^{(1)} <br />
		\Theta_{20}^{(1)} &amp; \Theta_{21}^{(1)} &amp; \Theta_{22}^{(1)} &amp; \Theta_{23}^{(1)} <br />
		\Theta_{30}^{(1)} &amp; \Theta_{31}^{(1)} &amp; \Theta_{32}^{(1)} &amp; \Theta_{33}^{(1)} 
	\end{bmatrix},  \tag{2}\label{eq:weight-matrix-1}   <br />
	\Theta^{(2)} &amp;= \begin{bmatrix}</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>\end{bmatrix}		
\end{align} $$
</code></pre></div></div>

:ET