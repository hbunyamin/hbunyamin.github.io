I"ý<p>This article explains <strong>forward propagation</strong> and <strong>backpropagation</strong> for one data instance in one round.</p>

<p>Suppose that we have an artificial neural network (ANN) for doing <em>binary classification</em> which consists of one hidden layer. Particularly, the hidden layer has 3 nodes or units with each has a <em>sigmoid activation function</em> ($\sigma$) as shown in the image below:</p>

<p><a href="/assets/images/ann-one-hidden.png"><img src="/assets/images/ann-one-hidden.png" alt="img1" class="img-responsive" /></a></p>

<p>Concretely, the input and hidden layer also have a <em>bias unit</em> respectively (not shown in the image). We stick to Andrewâ€™s notation from his excellent <a href="https://www.coursera.org/learn/machine-learning/home/welcome">Machine Learning Coursera</a>. Letâ€™s define our data instance</p>

\[\begin{equation}
		x = \begin{bmatrix}
			1 \\ 
			x_1 \\
			x_2 \\
			x_3
		\end{bmatrix}.\tag{1}\label{eq:data-instance}
	\end{equation}\]

<p>Our ANN also has the true label, $y$ and two weight matrices, $\Theta^{(1)}$ and $\Theta^{(2)}$ specified as follows:
\(\begin{align}
	\Theta^{(1)} &amp;= \begin{bmatrix}
		\Theta_{10}^{(1)} &amp; \Theta_{11}^{(1)} &amp; \Theta_{12}^{(1)} &amp; \Theta_{13}^{(1)} \\
		\Theta_{20}^{(1)} &amp; \Theta_{21}^{(1)} &amp; \Theta_{22}^{(1)} &amp; \Theta_{23}^{(1)} \\
		\Theta_{30}^{(1)} &amp; \Theta_{31}^{(1)} &amp; \Theta_{32}^{(1)} &amp; \Theta_{33}^{(1)} 
	\end{bmatrix} \text{ and} \tag{2}\label{eq:weight-matrix-1}   \\
	\Theta^{(2)} &amp;= \begin{bmatrix}
		\Theta_{10}^{(2)} &amp; \Theta_{11}^{(2)} &amp; \Theta_{12}^{(2)} &amp; \Theta_{13}^{(2)} 
	\end{bmatrix}. \tag{3}\label{eq:weight-matrix-2} 		
	\end{align}\)</p>

<p>Now firstly, let us compute for forward propagation. Here is the computation for the hidden layer.</p>

\[\begin{align} 
		\underbrace{z^{(2)}}_{3 \times 1} &amp;= \underbrace{\Theta^{(1)}}_{3 \times 4} \underbrace{x}_{4 \times 1} \tag{4}\label{eq:z-2} \\
		a^{(2)} &amp;= \sigma(z^{(2)})  \tag{5}\label{eq:a-2} \\
		a^{(2)} &amp;= \begin{bmatrix}
			1 \\
			a_1^{(2)} \\
			a_2^{(2)} \\
			a_3^{(2)}
		\end{bmatrix}  &amp; \text{add a bias unit.} \tag{6}\label{eq:a-2-bias}
	\end{align}\]

<p>Now we compute the output layer as follows:</p>

\[\begin{align} 
		\underbrace{z^{(3)}}_{1 \times 1} &amp;= \underbrace{\Theta^{(2)}}_{1 \times 4} \underbrace{a^{(2)}}_{4 \times 1} \tag{7}\label{eq:z-3} \\
		a^{(3)} &amp;= \sigma(z^{(3)})  \tag{8}\label{eq:a-3} \\
		h_\Theta(x) &amp;= a^{(3)}. \tag{9}\label{eq:h-x}
	\end{align}\]

<p>We have finished our <strong>forward propagation</strong>. Basically, forward propagation computes our modelâ€™s prediction. Now let us compute the backpropagation steps. Backpropagation algorithm calculates gradients; subsequently, the gradients are needed to update our weight matrices. Consequently, the updates should make our modelâ€™s current prediction better than the previous one.</p>

<p>Here is the backpropagation steps.</p>

\[\begin{align} 
		\underbrace{\delta^{(3)}}_{1 \times 1} &amp;= \underbrace{a^{(3)}}_{1 \times 1} - \underbrace{y}_{1 \times 1} \tag{10}\label{eq:delta-3} \\
		\underbrace{\delta^{(2)}}_{4 \times 1} &amp;= \underbrace{(\Theta^{(2)})^T}_{4 \times 1} \underbrace{\delta^{(3)}}_{1 \times 1} \odot \underbrace{a^{(2)}}_{4 \times 1} \odot \underbrace{(1 - a^{(2)})}_{4 \times 1} &amp; \odot = \text{element-wise multiplication} \tag{11}\label{eq:delta-2} \\
		\delta^{(2)} &amp;= \begin{bmatrix} 
			\delta_1^{(2)} \\
			\delta_2^{(2)} \\
			\delta_3^{(2)}
		\end{bmatrix} &amp; \text{remove }\delta_0^{(2)}. \tag{12}\label{eq:delta-2-remove-bias}
	\end{align}\]

<p>We have completed our <strong>backpropagation algorithm</strong>.  <br />
Finally we can update our weight matrices as follows:</p>

\[\begin{align}
		\Theta^{(2)} &amp;= \Theta^{(2)} - \alpha \times \delta^{(3)} (a^{(2)})^T &amp; \alpha = \text{learning rate} \tag{13}\label{eq:update-theta-2} \\
		\Theta^{(1)} &amp;= \Theta^{(1)} - \alpha \times \delta^{(2)} (x)^T. \tag{14}\label{eq:update-theta-1} 		
	\end{align}\]
:ET