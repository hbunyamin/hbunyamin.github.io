I"á<p>This article summarizes the topic ‚Äú<em><strong>Exponentially Weighted Moving Averages</strong></em>‚Äù (<a href="https://www.coursera.org/learn/deep-neural-network/home/week/2">Week 2 of <em>Improving Deep Neural Networks: Hyperparameters tuning, Regularization and Optimization</em></a>) from <a href="https://www.coursera.org/specializations/deep-learning?">deeplearning.ai</a>.</p>

<p>There are a few optimization algorithms which are faster than <em>gradient descent</em>. In order to understand these optimization algorithms, we need to understand the concept of <strong>exponentially weighted moving averages</strong>.</p>

<p><a href="/assets/images/temperature-and-days.png"><img src="/assets/images/temperature-and-days.png" alt="img1" class="img-responsive" /></a><em><center>A plot of temperatures for each day in a year. Image taken from <a href="https://www.coursera.org/learn/deep-neural-network/lecture/duStO/exponentially-weighted-averages">Deeplearning.ai</a>, some rights reserved.</center></em>
<br /></p>

<p>Suppose that the temperature of day $i$ is denoted by $\theta_i$ from $i=1, \ldots, 365$. Visually the temperatures are shown in the image above. Concretely, we show several examples of the temperatures as follows:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
	\theta_1 &= 40^{\circ} \text{F} \\
	\theta_2 &= 49^{\circ} \text{F} \\
	\theta_3 &= 40^{\circ} \text{F} \\	
	         &\vdots \\
	\theta_{180} &= 60^{\circ} \text{F} \\		         
	\theta_{181} &= 56^{\circ} \text{F} \\		      
			&\vdots
\end{align} %]]></script>

<p>As we can see in the image above, the values of temperatures during a year are <em>noisy</em> which means that there is considerable variation in the values. The variation is caused by <em>noise</em> and <em>we need to remove the variation</em> if we want to expose the underlying values of temperatures (Brownlee, 2019).</p>

<blockquote>
  <p>How do remove the noise which resides in the values of time series?</p>
</blockquote>

<p>One of the techniques to remove the noise is called <strong>smoothing</strong>. The common technique used commonly in time series forecasting is <strong>exponentially weighted averages</strong>. Computing <strong>exponentially weighted averages</strong> involves constructing a new series whose values are calculated by the average of raw observations in the original time series. Let‚Äôs denote the new series as $v_t$ for $t=1, 2, \ldots, 365$ as follows:</p>

<script type="math/tex; mode=display">\begin{equation}
 		v_t = \beta v_{t-1} + (1-\beta) \theta_t \tag{1}\label{eq:weighted-averages}. 
	\end{equation}</script>

<p>with $v_t$ is the average at time $t$, $\theta_t$ is the temperature at time $t$, and $\beta$ is the parameter determining <em>average number of days‚Äô temperatures</em>. Specifically,</p>

<script type="math/tex; mode=display">\begin{equation}
v_t \approx \pmb{\text{average }} \text{over } \frac{1}{1-\beta} \text{ days' temperature} \tag{2}\label{eq:parameter-beta}. 
\end{equation}</script>

<p>For example, let‚Äôs define $\beta = 0.9$ which means that we compute $v_t$ as approximately average over</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
	\frac{1}{1-\beta} &= \frac{1}{1-0.9} \\
	                  &= \frac{1}{0.1} \\
	                  &= 10 \text{ days}.
\end{align} %]]></script>

<p>Similarly, $\beta = 0.98$ gives approximately average over $50 \text{ days}$.
The plot of these weighted averages is shown below.</p>

<p><a href="/assets/images/moving-average-at-beta-0.9-and-0.98.png"><img src="/assets/images/moving-average-at-beta-0.9-and-0.98.png" alt="img1" class="img-responsive" /></a><em><center>A plot of $v_t$ at $\beta=0.9$ ($\pmb{\text{red}}$) and $\beta=0.98$ ($\pmb{\text{green}}$). Image taken from <a href="https://www.coursera.org/learn/deep-neural-network/lecture/duStO/exponentially-weighted-averages">Deeplearning.ai</a>, some rights reserved.</center></em>
<br /> <br />
As an extreme example, $\beta = 0.5$ computes approximately average over $2 \text{ days}$ as depicted in image below.</p>

<p><a href="/assets/images/moving-average-at-beta-0.5.png"><img src="/assets/images/moving-average-at-beta-0.5.png" alt="img1" class="img-responsive" /></a><em><center>A plot of $v_t$ at $\beta=0.5$ ($\pmb{\text{yellow}}$). Image taken from <a href="https://www.coursera.org/learn/deep-neural-network/lecture/duStO/exponentially-weighted-averages">Deeplearning.ai</a>, some rights reserved.</center></em>
<br /></p>

<p>From those three different values of $\beta$, we can conclude as follows:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
	\frac{1}{1-\beta} &= \frac{1}{1-0.9} \\
	                  &= \frac{1}{0.1} \\
	                  &= 10 \text{ days}.
\end{align} %]]></script>

<p>We have completed our <strong>backpropagation algorithm</strong>.  <br />
Finally we can update our weight matrices as follows:<br />
<br /></p>
<h4 id="references"><strong>References</strong></h4>
<p>Brownlee, J. (2019). <em>Introduction to Time Series Forecasting in Python</em>. <code class="highlighter-rouge">https://machinelearningmastery.com/introduction-to-time-series-forecasting-with-python/</code>. Accessed: 2020-09-5</p>
:ET