I"'<p>This article attempts to find gradients of a <a href="https://en.wikipedia.org/wiki/Softmax_function"><em>softmax</em></a> output layer. This knowledge proves useful when we want to utilize <em>backpropagation algorithm</em> to compute gradients of neural networks with a softmax output layer. Furthermore, <a href="https://drive.google.com/file/d/1UV_psOTNXZ0SB_-varbllZ79dLDSp5qU/view?usp=sharing">page 3 from the <strong>Notes on Backpropagation</strong> by <em>Peter Sadowski</em></a> has inspired this article a lot!</p>

<p>Suppose that we have a multiclass classification problem with 3 (three) choices that are label $1$, label $2$, and label $3$. Concretely, we utilize one-hot encoding for the three choices as follows:</p>

<p><script type="math/tex">\begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}</script>, <script type="math/tex">\begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}</script>, and <script type="math/tex">\begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}</script> are the representations for label $1$, label $2$, and label $3$ respectively.</p>

<p>Let us define our dataset, <script type="math/tex">X = \{ (x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \ldots, (x^{(m)}, y^{(m)}) \}</script>, which has <script type="math/tex">m</script> instances and</p>

<script type="math/tex; mode=display">\begin{equation} x^{(i)} = \begin{bmatrix} 1 \\ x_1^{(i)} \end{bmatrix} \text{ and }  y^{(i)} = \begin{bmatrix} y_1^{(i)} \\ y_2^{(i)} \\ y_3^{(i)} \end{bmatrix} \end{equation}\tag{1}\label{eq:label-y}</script>

<p>with <script type="math/tex">y_1^{(i)}</script>, <script type="math/tex">y_2^{(i)}</script>, <script type="math/tex">y_3^{(i)}</script> have only binary values (either 0 or 1) for <script type="math/tex">i = 1, 2, \ldots, m</script>.</p>

<p>We employ <a href="https://en.wikipedia.org/wiki/Softmax_function"><em>softmax</em></a> functions as our predictions. Specifically, we define our first hypoteses</p>

<script type="math/tex; mode=display">\begin{equation} h_1(x^{(i)}) = \frac{\exp{(\Theta_{10} + \Theta_{11} x_1^{(i)}})}{\exp{(\Theta_{10} + \Theta_{11} x_1^{(i)})}+\exp{(\Theta_{20} + \Theta_{21} x_1^{(i)})}+\exp{(\Theta_{30} + \Theta_{31} x_1^{(i)}})}
\end{equation}\tag{2}\label{eq:hyphotesis-1}</script>

<p>the second hyphotesis,</p>

<script type="math/tex; mode=display">\begin{equation} h_2(x^{(i)}) = \frac{\exp{(\Theta_{20} + \Theta_{21} x_1^{(i)})}}{\exp{(\Theta_{10} + \Theta_{11} x_1^{(i)})}+\exp{(\Theta_{20} + \Theta_{21} x_1^{(i)})}+\exp{(\Theta_{30} + \Theta_{31} x_1^{(i)}})}
\end{equation}\tag{3}\label{eq:hyphotesis-2}</script>

<p>the third hyphotesis,</p>

<script type="math/tex; mode=display">\begin{equation} h_3(x^{(i)}) = \frac{\exp{(\Theta_{30} + \Theta_{31} x_1^{(i)})}}{\exp{(\Theta_{10} + \Theta_{11} x_1^{(i)})}+\exp{(\Theta_{20} + \Theta_{21} x_1^{(i)})}+\exp{(\Theta_{30} + \Theta_{31} x_1^{(i)}})}
\end{equation}\tag{4}\label{eq:hyphotesis-4}</script>

<p>and the cost function,</p>

<script type="math/tex; mode=display">\begin{equation} J(\Theta) = - \sum_{i=1}^{m} ( y_1^{(i)} \log h_1(x^{(i)}) + y_2^{(i)} \log h_2(x^{(i)}) + y_3^{(i)} \log h_3(x^{(i)}) )
\end{equation}\tag{5}\label{eq:cost-function}</script>

<p>with</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation} \Theta = \begin{bmatrix} \Theta_{10} & \Theta_{11} \\
\Theta_{20} & \Theta_{21} \\
 \Theta_{30} & \Theta_{31} 
 \end{bmatrix}.
\end{equation}\tag{6}\label{eq:the-theta} %]]></script>

<p>Now we <strong>will show how to derive the gradient for these softmax activation function</strong>. In other words,</p>
<blockquote>
  <p>What are <script type="math/tex">\frac{\partial J}{\partial \Theta_{10}}</script>, <script type="math/tex">\frac{\partial J}{\partial \Theta_{11}}</script>, <script type="math/tex">\frac{\partial J}{\partial \Theta_{20}}</script>, <script type="math/tex">\frac{\partial J}{\partial \Theta_{21}}</script>, <script type="math/tex">\frac{\partial J}{\partial \Theta_{30}}</script>, and <script type="math/tex">\frac{\partial J}{\partial \Theta_{31}}</script>?</p>
</blockquote>

<p>Firstly, We show how to derive <script type="math/tex">\frac{\partial J}{\partial \Theta_{10}}</script> and <script type="math/tex">\frac{\partial J}{\partial \Theta_{11}}</script>.  <br />
<br /></p>
<h4 id="lets-derive-fracpmbpartial-jpmbpartial-theta_10"><strong>Letâ€™s derive <script type="math/tex">\frac{\pmb{\partial J}}{\pmb{\partial \Theta_{10}}}</script></strong></h4>
<p>By employing <a href="https://www.khanacademy.org/math/multivariable-calculus"><em>Multivariable Calculus</em></a>, we obtain</p>

<script type="math/tex; mode=display">\begin{equation} \frac{\partial J}{\partial \Theta_{10}} = \underbrace{\frac{\partial J}{\partial h_1}\frac{\partial h_1}{\partial \Theta_{10}}}_{\text{Part I}} + \underbrace{\frac{\partial J}{\partial h_2}\frac{\partial h_2}{\partial \Theta_{10}}}_{\text{Part II}} + \underbrace{\frac{\partial J}{\partial h_3}\frac{\partial h_3}{\partial \Theta_{10}}}_{\text{Part III}}.
\end{equation}\tag{7}\label{eq:gradient-10}</script>

<p>The Part I consists of <script type="math/tex">\frac{\partial J}{\partial h_1}\frac{\partial h_1}{\partial \Theta_{10}}</script>. Specifically,</p>

<script type="math/tex; mode=display">\begin{equation} \frac{\partial J}{\partial h_1} = - \sum_{i=1}^{m}{\frac{y_1^{(i)}}{h_1(x^{(i)})}}.
\end{equation}\tag{8}\label{eq:gradient-10-1}</script>

<p>By defining</p>

<script type="math/tex; mode=display">\begin{equation} u =  \exp{(\Theta_{10} + \Theta_{11}x_1^{(i)})}
\end{equation}\tag{9}\label{eq:gradient-10-2}</script>

<p>and</p>

<script type="math/tex; mode=display">\begin{equation} v =  \exp{(\Theta_{10} + \Theta_{11}x_1^{(i)})} + \exp{(\Theta_{20} + \Theta_{21}x_1^{(i)})} + \exp{(\Theta_{30} + \Theta_{31}x_1^{(i)})}
\end{equation}\tag{10}\label{eq:gradient-10-3}</script>

<p>and <a href="https://www.khanacademy.org/math/ap-calculus-ab/ab-differentiation-1-new/ab-2-9/v/quotient-rule">Quotient Rule</a>, we are able to compute <script type="math/tex">\frac{\partial h_1}{\partial \Theta_{10}}</script> as follows:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align} \frac{\partial h_1}{\partial \Theta_{10}} &= \frac{u^{\prime} v - u v^{\prime}}{v^2} \\
&= \frac{(\exp{(\Theta_{10} + \Theta_{11}x_1^{(i)})})(\exp{(\Theta_{10} + \Theta_{11}x_1^{(i)})} + \exp{(\Theta_{20} + \Theta_{21}x_1^{(i)})} + \exp{(\Theta_{30} + \Theta_{31}x_1^{(i)})}) - (\exp{(\Theta_{10} + \Theta_{11}x_1^{(i)})})^2}{(\exp{(\Theta_{10} + \Theta_{11}x_1^{(i)})} + \exp{(\Theta_{20} + \Theta_{21}x_1^{(i)})} + \exp{(\Theta_{30} + \Theta_{31}x_1^{(i)})})^2} \\
&= \frac{\exp{(\Theta_{10} + \Theta_{11}x_1^{(i)})}}{\exp{(\Theta_{10} + \Theta_{11}x_1^{(i)})} + \exp{(\Theta_{20} + \Theta_{21}x_1^{(i)})} + \exp{(\Theta_{30} + \Theta_{31}x_1^{(i)})}} - \left( \frac{\exp{(\Theta_{10} + \Theta_{11}x_1^{(i)})}}{\exp{(\Theta_{10} + \Theta_{11}x_1^{(i)})} + \exp{(\Theta_{20} + \Theta_{21}x_1^{(i)})} + \exp{(\Theta_{30} + \Theta_{31}x_1^{(i)})}} \right)^2 \\
&= h_1(x^{(i)}) - (h_1(x^{(i)}))^2 \\
&= h_1(x^{(i)}) (1 - h_1(x^{(i)}))\tag{11}\label{eq:gradient-10-4}
\end{align} %]]></script>

<p>Finally, we can compute <script type="math/tex">\frac{\partial J}{\partial h_1}\frac{\partial h_1}{\partial \Theta_{10}}</script> by combining Equation \eqref{eq:gradient-10-1} and Equation \eqref{eq:gradient-10-4} as follows:</p>

<script type="math/tex; mode=display">% <![CDATA[
\require{cancel} \begin{align} \frac{\partial J}{\partial h_1}\frac{\partial h_1}{\partial \Theta_{10}} &= - \sum_{i=1}^{m}{\frac{y_1^{(i)}}{\cancel{h_1(x^{(i)})}} \cancel{h_1(x^{(i)})} (1 - h_1(x^{(i)}) )} \\
      &= - \sum_{i=1}^{m}{y_1^{(i)} (1 - h_1(x^{(i)}))}\tag{12}\label{eq:gradient-10-5}
\end{align} %]]></script>

<p>The Part II consists of <script type="math/tex">\frac{\partial J}{\partial h_2}\frac{\partial h_2}{\partial \Theta_{10}}</script>. Specifically,</p>

<script type="math/tex; mode=display">\begin{equation} \frac{\partial J}{\partial h_2} = - \sum_{i=1}^{m}{\frac{y_2^{(i)}}{h_2(x^{(i)})}}.
\end{equation}\tag{13}\label{eq:gradient-10-6}</script>

<p>Again, by defining</p>

<script type="math/tex; mode=display">\begin{equation} u =  \exp{(\Theta_{20} + \Theta_{21}x_1^{(i)})}
\end{equation},\tag{14}\label{eq:gradient-10-7}</script>

<p>using Equation \eqref{eq:gradient-10-3}, and <a href="https://www.khanacademy.org/math/ap-calculus-ab/ab-differentiation-1-new/ab-2-9/v/quotient-rule">Quotient Rule</a>, we can compute <script type="math/tex">\frac{\partial h_2}{\partial \Theta_{10}}</script></p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align} \frac{\partial h_2}{\partial \Theta_{10}} &= \frac{u^{\prime} v - u v^{\prime}}{v^2} \\
&= \frac{0 - \exp{(\Theta_{20} + \Theta_{21} x_1^{(i)})} \exp{(\Theta_{10} + \Theta_{11} x_1^{(i)})}}{(\exp{(\Theta_{10} + \Theta_{11}x_1^{(i)})} + \exp{(\Theta_{20} + \Theta_{21}x_1^{(i)})} + \exp{(\Theta_{30} + \Theta_{31}x_1^{(i)})})^2} \\
&= - \left(\frac{\exp{(\Theta_{20} + \Theta_{21} x_1^{(i)})}}{\exp{(\Theta_{10} + \Theta_{11}x_1^{(i)})} + \exp{(\Theta_{20} + \Theta_{21}x_1^{(i)})} + \exp{(\Theta_{30} + \Theta_{31}x_1^{(i)})}} \right) \left( \frac{\exp{(\Theta_{10} + \Theta_{11} x_1^{(i)})}}{\exp{(\Theta_{10} + \Theta_{11}x_1^{(i)})} + \exp{(\Theta_{20} + \Theta_{21}x_1^{(i)})} + \exp{(\Theta_{30} + \Theta_{31}x_1^{(i)})}} \right) \\
&= - h_2(x^{(i)}) h_1(x^{(i)}) \tag{15}\label{eq:gradient-10-8}
\end{align} %]]></script>

<p>By using Equation \eqref{eq:gradient-10-6} and Equation \eqref{eq:gradient-10-8},  <script type="math/tex">\frac{\partial J}{\partial h_2}\frac{\partial h_2}{\partial \Theta_{10}}</script> can be computed as</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align} \frac{\partial J}{\partial h_2}\frac{\partial h_2}{\partial \Theta_{10}} &= + \sum_{i=1}^{m}{\frac{y_2^{(i)}}{\cancel{h_2(x^{(i)})}} \cancel{h_2(x^{(i)})} h_1(x^{(i)})} \\
      &= - \sum_{i=1}^{m}{y_1^{(i)} (1 - h_1(x^{(i)}))}\tag{16}\label{eq:gradient-10-9}
\end{align} %]]></script>

:ET