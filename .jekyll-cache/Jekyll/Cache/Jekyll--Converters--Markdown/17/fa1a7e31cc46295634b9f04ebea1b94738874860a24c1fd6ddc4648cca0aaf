I"D<p>This article explains <strong>forward propagation</strong> and <strong>backpropagation</strong> for one data instance in one round.</p>

<p>Suppose that we have an artificial neural network (ANN) for doing <em>binary classification</em> which consists of one hidden layer. Particularly, the hidden layer has 3 nodes or units as shown in the image below:</p>

<p><a href="/assets/images/ann-one-hidden.png"><img src="/assets/images/ann-one-hidden.png" alt="img1" class="img-responsive" /></a></p>

<p>Concretely, the input and hidden layer also have a <em>bias unit</em> respectively (not shown in the image). We stick to Andrew’s notation from his excellent <a href="https://www.coursera.org/learn/machine-learning/home/welcome">Machine Learning Coursera</a>. Let’s define our data instance</p>

<script type="math/tex; mode=display">\begin{equation}
		x = \begin{bmatrix}
			1 \\ 
			x_1 \\
			x_2 \\
			x_3
		\end{bmatrix}.\tag{1}\label{eq:data-instance}
	\end{equation}</script>

<p>Our ANN also has two weight matrices, $\Theta^{(1)}$ and $\Theta^{(2)}$ specified as follows:</p>

:ET