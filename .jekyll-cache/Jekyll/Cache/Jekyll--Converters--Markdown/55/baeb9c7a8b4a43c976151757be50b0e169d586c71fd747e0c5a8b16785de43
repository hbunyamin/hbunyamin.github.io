I"—<p>This article is inspired by <a href="https://eli.thegreenplace.net/2015/the-normal-equation-and-matrix-calculus">an excellent post by Eli Bendersky</a>. Letâ€™s continue with the derivation.</p>

<p><strong>Cost function</strong> has been explained in <a href="https://www.coursera.org/learn/machine-learning/home/week/1">Week 1</a> and <a href="https://www.coursera.org/learn/machine-learning/home/week/2">Week 2</a> of <a href="https://www.coursera.org/learn/machine-learning/home/welcome"><em>Machine Learning</em> course taught by Andrew Ng</a>. This post tries to explain how to derive <strong>normal equation</strong> for <em>linear regression with multiple variables</em>. It is a good thing if all readers has studied <a href="https://www.coursera.org/learn/machine-learning/home/week/1">Week 1</a> and <a href="https://www.coursera.org/learn/machine-learning/home/week/2">Week 2</a> before reading this post.</p>

<p>The <strong>cost function</strong> of <em>linear regression with multiple variables</em>, $J(\theta)$ is formulated as follows:</p>

\[\begin{equation} J(\theta) = \frac{1}{2m} \sum_{i=1}^{m}{(h_{\theta}(x^{(i)}) - y^{(i)})^2}  \tag{1}\label{eq:cost-function} \end{equation}\]

<p>with $m$ is number of instances in dataset, $h_{\theta}(x^{(i)})$ is our hyphotesis also known as prediction model for the $i$th instance, and $y^{(i)}$ is true value for the $i$th instance.</p>

<p>We also have studied that</p>

\[\begin{equation} h_{\theta}(x^{(i)}) = \theta_0 + \theta_1 x_1^{(i)} + \cdots + \theta_n x_n^{(i)}  \tag{2}\label{eq:the-hyphotesis} \end{equation}\]

<p>By substituting \eqref{eq:the-hyphotesis} into \eqref{eq:cost-function}, we obtain</p>

\[\begin{align}  J(\theta) &amp;= \frac{1}{2m} \sum_{i=1}^{m}{(\theta_0 + \theta_1 x_1^{(i)} + \cdots + \theta_n x_n^{(i)} - y^{(i)})^2}  \tag{3}\label{eq:derivation-1} \\
&amp;= \frac{1}{2m} ((\theta_0 + \theta_1 x_1^{(1)} + \cdots + \theta_n x_n^{(1)} - y^{(1)} )^2 + \cdots + (\theta_0 + \theta_1 x_1^{(m)} + \cdots + \theta_n x_n^{(m)} - y^{(m)} )^2 ) \tag{4}\label{eq:derivation-2} \\
&amp;= \frac{1}{2m} \underbrace{ \begin{bmatrix} (\theta_0 + \theta_1 x_1^{(1)} + \cdots + \theta_n x_n^{(1)} - y^{(1)}) &amp; \cdots &amp; (\theta_0 + \theta_1 x_1^{(m)} + \cdots + \theta_n x_n^{(m)} - y^{(m)}) \end{bmatrix}}_{\text{matrix with size: } 1 \times m} \underbrace{\begin{bmatrix} (\theta_0 +\theta_1 x_1^{(1)} + \cdots + \theta_n x_n^{(1)} - y^{(1)}) \\
\vdots \\
(\theta_0 +\theta_1 x_1^{(m)} + \cdots + \theta_n x_n^{(m)} - y^{(m)})
 \end{bmatrix}}_{\text{matrix with size: } m \times 1} \tag{5}\label{eq:derivation-3} \\
 &amp;= \frac{1}{2m} \left( \begin{bmatrix} (\theta_0 + \theta_1 x_1^{(1)} + \cdots + \theta_n x_n^{(1)} ) &amp; \cdots &amp; (\theta_0 + \theta_1 x_1^{(m)} + \cdots + \theta_n x_n^{(m)} ) \end{bmatrix} - \begin{bmatrix} y^{(1)} &amp; \cdots &amp; y^{(m)} \end{bmatrix} \right) \left( \begin{bmatrix} \theta_0 + \theta_1 x_1^{(1)} + \cdots + \theta_n x_n^{(1)} \\
 \theta_0 + \theta_1 x_1^{(2)} + \cdots + \theta_n x_n^{(2)} \\
 \vdots \\
 \theta_0 + \theta_1 x_1^{(m)} + \cdots + \theta_n x_n^{(m)} \end{bmatrix}  - \begin{bmatrix} y^{(1)} \\ 
 y^{(2)} \\
 \vdots \\
 y^{(m)} \end{bmatrix} \right) \tag{6}\label{eq:derivation-4} \\
 &amp;= \frac{1}{2m} \left( \begin{bmatrix} \theta_0 &amp; \theta_1 &amp; \cdots &amp; \theta_n \end{bmatrix} \begin{bmatrix} 1 &amp; 1 &amp; \cdots &amp; 1 \\ 
 x_1^{(1)} &amp; x_1^{(2)} &amp; \cdots &amp; x_1^{(m)} \\
 \vdots &amp; \vdots &amp; \cdots &amp; \vdots \\
 x_n^{(1)} &amp; x_n^{(2)} &amp; \cdots &amp; x_n^{(m)} \\\end{bmatrix} - \begin{bmatrix} y^{(1)} &amp; \cdots &amp; y^{(m)} \end{bmatrix} \right) \left( \begin{bmatrix} 1 &amp; x_1^{(1)} &amp; \cdots &amp; x_n^{(1)} \\
  1 &amp; x_1^{(2)} &amp; \cdots &amp; x_n^{(2)} \\
  \vdots &amp; \vdots &amp; \cdots &amp; \vdots \\
  1 &amp; x_1^{(m)} &amp; \cdots &amp; x_n^{(m)} \end{bmatrix} \begin{bmatrix} \theta_0 \\ 
 \theta_1 \\
 \vdots \\
 \theta_n \end{bmatrix} - \begin{bmatrix} y^{(1)} \\
  y^{(2)} \\ 
  \ldots \\
  y^{(m)} \end{bmatrix} \right) \tag{7}\label{eq:derivation-5}  \end{align}\]

<p>By defining</p>

\[\begin{equation} \theta = \begin{bmatrix} \theta_0 \\ 
\theta_1 \\ 
\vdots \\
\theta_n \end{bmatrix} \tag{8}\label{eq:defining-theta} \end{equation}\]

<p>and</p>

\[\begin{equation} X = \begin{bmatrix} 1 &amp; x_1^{(1)} &amp; \cdots &amp; x_n^{(1)} \\
1      &amp; x_1^{(2)} &amp; \cdots  &amp; x_n^{(2)} \\ 
\vdots &amp; \vdots    &amp;  \vdots &amp;  \vdots  \\
1 &amp; x_1^{(m)} &amp; \cdots &amp; x_n^{(m)} \end{bmatrix} \tag{9}\label{eq:defining-X} \end{equation}\]

<p>also</p>

\[\begin{equation} Y = \begin{bmatrix} y^{(1)} \\ 
y^{(2)} \\ 
\vdots \\
y^{(m)} \end{bmatrix}, \tag{10}\label{eq:defining-Y} \end{equation}\]

<p>equation \eqref{eq:derivation-5} becomes</p>

\[\begin{align} J(\theta) &amp;= \frac{1}{2m} (\theta^T X^T - Y^T)(X \theta - Y) &amp;&amp;\text{by transpose property} \tag{11}\label{eq:derivation-6} \\
&amp;= \frac{1}{2m} (\theta^T X^T X \theta - \underbrace{\theta^T X^T Y}_{\text{a scalar}} - \underbrace{Y^T X \theta}_{\text{a scalar}} + Y^T Y)  &amp;&amp;\text{by matrix multiplication} \tag{12}\label{eq:derivation-7} \\
&amp;= \frac{1}{2m} (\theta^T X^T X \theta - \theta^T X^T Y - (X \theta)^T Y + Y^T Y) &amp;&amp;\text{by rearranging a scalar} \tag{13}\label{eq:derivation-8} \\
&amp;= \frac{1}{2m} (\theta^T X^T X \theta - \theta^T X^T Y - \theta^T X^T Y + Y^T Y) &amp;&amp;\text{by transpose property} \tag{14}\label{eq:derivation-9} \\
&amp;= \frac{1}{2m} (\underbrace{\theta^T X^T X \theta}_{\text{Part I}} - \underbrace{2 \theta^T X^T Y}_{\text{Part II}} + Y^T Y) &amp;&amp;\text{by summation property} \tag{15}\label{eq:derivation-10} \\
\end{align}\]

<p>We have arrived into a matrix form from <strong>linear regression cost function</strong>. Our next step would be:</p>

<blockquote>
  <p>How can we minimize the <strong>cost function</strong> in Equation \eqref{eq:derivation-10}?</p>
</blockquote>

<p>We will employ the derivation formula from <a href="https://en.wikipedia.org/wiki/Matrix_calculus">Matrix Calculus</a>; specifically, we use <strong>two scalar-by-vector identities</strong> with <strong>denominator layout</strong> (result: column vector). The identities are as follows:</p>

\[\begin{equation} 
	\frac{\partial \mathbf{x}^T \mathbf{A} \mathbf{x} }{\partial \mathbf{x}} = 2 \mathbf{A} \mathbf{x} \tag{16}\label{eq:identity-1}
\end{equation}\]

<p>and</p>

\[\begin{equation} 
	\frac{\partial \mathbf{a}^T \mathbf{x} }{\partial \mathbf{x}} = \frac{\partial \mathbf{x}^T \mathbf{a} }{\partial \mathbf{x}} = \mathbf{a} \tag{17}\label{eq:identity-2}
\end{equation}\]

<p>Now equipped with these identities, let us minimize Equation \eqref{eq:derivation-10} by computing the first derivation of $J(\theta)$; specifically, the Part I is computed with Equation \eqref{eq:identity-1} and Part II with Equation \eqref{eq:identity-2}:</p>

\[\begin{equation} 
	\frac{\partial J }{\partial \theta} = \frac{1}{2m} ( 2 X^T X \theta - 2 X^T Y ) \tag{18}\label{eq:derivation-of-J}
\end{equation}\]

<p>In order to find $\theta$ which minimize Equation \eqref{eq:derivation-10}, we need to solve</p>

\[\begin{align} \frac{\partial J}{\partial \theta} = 0 &amp;\Longleftrightarrow \frac{1}{2m} ( 2 X^T X \theta - 2 X^T Y ) = 0 \\
 &amp;\Longleftrightarrow 2 X^T X \theta - 2 X^T Y = 0 \\
   &amp;\Longleftrightarrow 2 X^T X \theta = 2 X^T Y  \\ 
   &amp;\Longleftrightarrow X^T X \theta = X^T Y \\ 
   &amp;\Longleftrightarrow \theta = (X^T X)^{-1} X^T Y &amp;&amp;\text{by inverse matrix} \tag{19}\label{eq:final-result} \end{align}\]

<p>At last, we have derived <strong>the normal equation of linear regression model</strong> that is</p>

\[\begin{equation} \bbox[5px,border:2px solid blue] {\theta = (X^T X)^{-1} X^T Y}. \end{equation}\]
:ET