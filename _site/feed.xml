<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2019-10-20T20:02:47+07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Hendra Bunyamin</title><subtitle>Forgiven sinner and Lecturer at Maranatha Christian University</subtitle><entry xml:lang="EN"><title type="html">Implementing Gibbs Sampler for the Uninitiated</title><link href="http://localhost:4000/machine-learning/Gibbs_Sampler_for_Naive_Bayes/" rel="alternate" type="text/html" title="Implementing Gibbs Sampler for the Uninitiated" /><published>2019-10-08T00:00:00+07:00</published><updated>2019-10-08T00:00:00+07:00</updated><id>http://localhost:4000/machine-learning/Gibbs_Sampler_for_Naive_Bayes</id><content type="html" xml:base="http://localhost:4000/machine-learning/Gibbs_Sampler_for_Naive_Bayes/">&lt;p&gt;This article is inspired by &lt;a href=&quot;https://www.umiacs.umd.edu/~resnik/pubs/LAMP-TR-153.pdf&quot;&gt;a masterpiece exposition on Gibbs Sampler by Resnik and Hardisty&lt;/a&gt; and [an outstanding github repo]. The articleâ€™s title is Gibbs&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Cost function&lt;/strong&gt; has been explained in &lt;a href=&quot;https://www.coursera.org/learn/machine-learning/home/week/1&quot;&gt;Week 1&lt;/a&gt; and &lt;a href=&quot;https://www.coursera.org/learn/machine-learning/home/week/2&quot;&gt;Week 2&lt;/a&gt; of &lt;a href=&quot;https://www.coursera.org/learn/machine-learning/home/welcome&quot;&gt;&lt;em&gt;Machine Learning&lt;/em&gt; course taught by Andrew Ng&lt;/a&gt;. This post tries to explain how to derive &lt;strong&gt;normal equation&lt;/strong&gt; for &lt;em&gt;linear regression with multiple variables&lt;/em&gt;. It is a good thing if all readers has studied &lt;a href=&quot;https://www.coursera.org/learn/machine-learning/home/week/1&quot;&gt;Week 1&lt;/a&gt; and &lt;a href=&quot;https://www.coursera.org/learn/machine-learning/home/week/2&quot;&gt;Week 2&lt;/a&gt; before reading this post.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;cost function&lt;/strong&gt; of &lt;em&gt;linear regression with multiple variables&lt;/em&gt;, $J(\theta)$ is formulated as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation} J(\theta) = \frac{1}{2m} \sum_{i=1}^{m}{(h_{\theta}(x^{(i)}) - y^{(i)})^2}  \tag{1}\label{eq:cost-function} \end{equation}&lt;/script&gt;

&lt;p&gt;with $m$ is number of instances in dataset, $h_{\theta}(x^{(i)})$ is our hyphotesis also known as prediction model for the $i$th instance, and $y^{(i)}$ is true value for the $i$th instance.&lt;/p&gt;

&lt;p&gt;We also have studied that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation} h_{\theta}(x^{(i)}) = \theta_0 + \theta_1 x_1^{(i)} + \cdots + \theta_n x_n^{(i)}  \tag{2}\label{eq:the-hyphotesis} \end{equation}&lt;/script&gt;

&lt;p&gt;By substituting \eqref{eq:the-hyphotesis} into \eqref{eq:cost-function}, we obtain&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}  J(\theta) &amp;= \frac{1}{2m} \sum_{i=1}^{m}{(\theta_0 + \theta_1 x_1^{(i)} + \cdots + \theta_n x_n^{(i)} - y^{(i)})^2}  \tag{3}\label{eq:derivation-1} \\
&amp;= \frac{1}{2m} ((\theta_0 + \theta_1 x_1^{(1)} + \cdots + \theta_n x_n^{(1)} - y^{(1)} )^2 + \cdots + (\theta_0 + \theta_1 x_1^{(m)} + \cdots + \theta_n x_n^{(m)} - y^{(m)} )^2 ) \tag{4}\label{eq:derivation-2} \\
&amp;= \frac{1}{2m} \underbrace{ \begin{bmatrix} (\theta_0 + \theta_1 x_1^{(1)} + \cdots + \theta_n x_n^{(1)} - y^{(1)}) &amp; \cdots &amp; (\theta_0 + \theta_1 x_1^{(m)} + \cdots + \theta_n x_n^{(m)} - y^{(m)}) \end{bmatrix}}_{\text{matrix with size: } 1 \times m} \underbrace{\begin{bmatrix} (\theta_0 +\theta_1 x_1^{(1)} + \cdots + \theta_n x_n^{(1)} - y^{(1)}) \\
\vdots \\
(\theta_0 +\theta_1 x_1^{(m)} + \cdots + \theta_n x_n^{(m)} - y^{(1)})
 \end{bmatrix}}_{\text{matrix with size: } m \times 1} \tag{5}\label{eq:derivation-3} \\
 &amp;= \frac{1}{2m} \left( \begin{bmatrix} (\theta_0 + \theta_1 x_1^{(1)} + \cdots + \theta_n x_n^{(1)} - y^{(1)}) &amp; \cdots &amp; (\theta_0 + \theta_1 x_1^{(m)} + \cdots + \theta_n x_n^{(m)} - y^{(m)}) \end{bmatrix} - \begin{bmatrix} y^{(1)} &amp; \cdots &amp; y^{(m)} \end{bmatrix} \right) \left( \begin{bmatrix} \theta_0 + \theta_1 x_1^{(1)} + \cdots + \theta_n x_n^{(1)} \\
 \theta_0 + \theta_1 x_1^{(2)} + \cdots + \theta_n x_n^{(2)} \\
 \vdots \\
 \theta_0 + \theta_1 x_1^{(m)} + \cdots + \theta_n x_n^{(m)} \end{bmatrix}  - \begin{bmatrix} y^{(1)} \\ 
 y^{(2)} \\
 \vdots \\
 y^{(m)} \end{bmatrix} \right) \tag{6}\label{eq:derivation-4} \\
 &amp;= \frac{1}{2m} \left( \begin{bmatrix} \theta_0 &amp; \theta_1 &amp; \cdots &amp; \theta_n \end{bmatrix} \begin{bmatrix} 1 &amp; 1 &amp; \cdots &amp; 1 \\ 
 x_1^{(1)} &amp; x_1^{(2)} &amp; \cdots &amp; x_1^{(m)} \\
 \vdots &amp; \vdots &amp; \cdots &amp; \vdots \\
 x_n^{(1)} &amp; x_n^{(2)} &amp; \cdots &amp; x_n^{(m)} \\\end{bmatrix} - \begin{bmatrix} y^{(1)} &amp; \cdots &amp; y^{(m)} \end{bmatrix} \right) \left( \begin{bmatrix} 1 &amp; x_1^{(1)} &amp; \cdots &amp; x_n^{(1)} \\
  1 &amp; x_1^{(2)} &amp; \cdots &amp; x_n^{(2)} \\
  \vdots &amp; \vdots &amp; \cdots &amp; \vdots \\
  1 &amp; x_1^{(m)} &amp; \cdots &amp; x_n^{(m)} \end{bmatrix} \begin{bmatrix} \theta_0 \\ 
 \theta_1 \\
 \vdots \\
 \theta_n \end{bmatrix} - \begin{bmatrix} y^{(1)} \\
  y^{(2)} \\ 
  \ldots \\
  y^{(m)} \end{bmatrix} \right) \tag{7}\label{eq:derivation-5}  \end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;By defining&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation} \theta = \begin{bmatrix} \theta_0 \\ 
\theta_1 \\ 
\vdots \\
\theta_n \end{bmatrix} \tag{8}\label{eq:defining-theta} \end{equation}&lt;/script&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation} X = \begin{bmatrix} 1 &amp; x_1^{(1)} &amp; \cdots &amp; x_n^{(1)} \\
1      &amp; x_1^{(2)} &amp; \cdots  &amp; x_n^{(2)} \\ 
\vdots &amp; \vdots    &amp;  \vdots &amp;  \vdots  \\
1 &amp; x_1^{(m)} &amp; \cdots &amp; x_n^{(m)} \end{bmatrix} \tag{9}\label{eq:defining-X} \end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;also&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation} Y = \begin{bmatrix} y^{(1)} \\ 
y^{(2)} \\ 
\vdots \\
y^{(m)} \end{bmatrix}, \tag{10}\label{eq:defining-Y} \end{equation}&lt;/script&gt;

&lt;p&gt;equation \eqref{eq:derivation-5} becomes&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align} J(\theta) &amp;= \frac{1}{2m} (\theta^T X^T - Y^T)(X \theta - Y) &amp;&amp;\text{by transpose property} \tag{11}\label{eq:derivation-6} \\
&amp;= \frac{1}{2m} (\theta^T X^T X \theta - \underbrace{\theta^T X^T Y}_{\text{a scalar}} - \underbrace{Y^T X \theta}_{\text{a scalar}} + Y^T Y)  &amp;&amp;\text{by matrix multiplication} \tag{12}\label{eq:derivation-7} \\
&amp;= \frac{1}{2m} (\theta^T X^T X \theta - \theta^T X^T Y - (X \theta)^T Y + Y^T Y) &amp;&amp;\text{by rearranging a scalar} \tag{13}\label{eq:derivation-8} \\
&amp;= \frac{1}{2m} (\theta^T X^T X \theta - \theta^T X^T Y - \theta^T X^T Y + Y^T Y) &amp;&amp;\text{by transpose property} \tag{14}\label{eq:derivation-9} \\
&amp;= \frac{1}{2m} (\underbrace{\theta^T X^T X \theta}_{\text{Part I}} - \underbrace{2 \theta^T X^T Y}_{\text{Part II}} + Y^T Y) &amp;&amp;\text{by summation property} \tag{15}\label{eq:derivation-10} \\
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;We have arrived into a matrix form from &lt;strong&gt;linear regression cost function&lt;/strong&gt;. Our next step would be:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;How can we minimize the &lt;strong&gt;cost function&lt;/strong&gt; in Equation \eqref{eq:derivation-10}?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;We will employ the derivation formula from &lt;a href=&quot;https://en.wikipedia.org/wiki/Matrix_calculus&quot;&gt;Matrix Calculus&lt;/a&gt;; specifically, we use &lt;strong&gt;two scalar-by-vector identities&lt;/strong&gt; with &lt;strong&gt;denominator layout&lt;/strong&gt; (result: column vector). The identities are as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation} 
	\frac{\partial \mathbf{x}^T \mathbf{A} \mathbf{x} }{\partial \mathbf{x}} = 2 \mathbf{A} \mathbf{x} \tag{16}\label{eq:identity-1}
\end{equation}&lt;/script&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation} 
	\frac{\partial \mathbf{a}^T \mathbf{x} }{\partial \mathbf{x}} = \frac{\partial \mathbf{x}^T \mathbf{a} }{\partial \mathbf{x}} = \mathbf{a} \tag{17}\label{eq:identity-2}
\end{equation}&lt;/script&gt;

&lt;p&gt;Now equipped with these identities, let us minimize Equation \eqref{eq:derivation-10} by computing the first derivation of $J(\theta)$; specifically, the Part I is computed with Equation \eqref{eq:identity-1} and Part II with Equation \eqref{eq:identity-2}:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation} 
	\frac{\partial J }{\partial \theta} = \frac{1}{2m} ( 2 X^T X \theta - 2 X^T Y ) \tag{18}\label{eq:derivation-of-J}
\end{equation}&lt;/script&gt;

&lt;p&gt;In order to find $\theta$ which minimize Equation \eqref{eq:derivation-10}, we need to solve&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align} \frac{\partial J}{\partial \theta} = 0 &amp;\Longleftrightarrow \frac{1}{2m} ( 2 X^T X \theta - 2 X^T Y ) = 0 \\
 &amp;\Longleftrightarrow 2 X^T X \theta - 2 X^T Y = 0 \\
   &amp;\Longleftrightarrow 2 X^T X \theta = 2 X^T Y  \\ 
   &amp;\Longleftrightarrow X^T X \theta = X^T Y \\ 
   &amp;\Longleftrightarrow \theta = (X^T X)^{-1} X^T Y &amp;&amp;\text{by inverse matrix} \tag{19}\label{eq:final-result} \end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;At last, we have derived &lt;strong&gt;the normal equation of linear regression model&lt;/strong&gt; that is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation} \bbox[5px,border:2px solid blue] {\theta = (X^T X)^{-1} X^T Y}. \end{equation}&lt;/script&gt;</content><author><name></name></author><category term="Machine-Learning" /><summary type="html">This article is inspired by a masterpiece exposition on Gibbs Sampler by Resnik and Hardisty and [an outstanding github repo]. The articleâ€™s title is Gibbs</summary></entry><entry xml:lang="EN"><title type="html">Deriving Normal Equation of Linear Regression Model</title><link href="http://localhost:4000/machine-learning/Normal_equation/" rel="alternate" type="text/html" title="Deriving Normal Equation of Linear Regression Model" /><published>2019-08-18T00:00:00+07:00</published><updated>2019-08-18T00:00:00+07:00</updated><id>http://localhost:4000/machine-learning/Normal_equation</id><content type="html" xml:base="http://localhost:4000/machine-learning/Normal_equation/">&lt;p&gt;This article is inspired by &lt;a href=&quot;https://eli.thegreenplace.net/2015/the-normal-equation-and-matrix-calculus&quot;&gt;an excellent post by Eli Bendersky&lt;/a&gt;. Letâ€™s continue with the derivation.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Cost function&lt;/strong&gt; has been explained in &lt;a href=&quot;https://www.coursera.org/learn/machine-learning/home/week/1&quot;&gt;Week 1&lt;/a&gt; and &lt;a href=&quot;https://www.coursera.org/learn/machine-learning/home/week/2&quot;&gt;Week 2&lt;/a&gt; of &lt;a href=&quot;https://www.coursera.org/learn/machine-learning/home/welcome&quot;&gt;&lt;em&gt;Machine Learning&lt;/em&gt; course taught by Andrew Ng&lt;/a&gt;. This post tries to explain how to derive &lt;strong&gt;normal equation&lt;/strong&gt; for &lt;em&gt;linear regression with multiple variables&lt;/em&gt;. It is a good thing if all readers has studied &lt;a href=&quot;https://www.coursera.org/learn/machine-learning/home/week/1&quot;&gt;Week 1&lt;/a&gt; and &lt;a href=&quot;https://www.coursera.org/learn/machine-learning/home/week/2&quot;&gt;Week 2&lt;/a&gt; before reading this post.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;cost function&lt;/strong&gt; of &lt;em&gt;linear regression with multiple variables&lt;/em&gt;, $J(\theta)$ is formulated as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation} J(\theta) = \frac{1}{2m} \sum_{i=1}^{m}{(h_{\theta}(x^{(i)}) - y^{(i)})^2}  \tag{1}\label{eq:cost-function} \end{equation}&lt;/script&gt;

&lt;p&gt;with $m$ is number of instances in dataset, $h_{\theta}(x^{(i)})$ is our hyphotesis also known as prediction model for the $i$th instance, and $y^{(i)}$ is true value for the $i$th instance.&lt;/p&gt;

&lt;p&gt;We also have studied that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation} h_{\theta}(x^{(i)}) = \theta_0 + \theta_1 x_1^{(i)} + \cdots + \theta_n x_n^{(i)}  \tag{2}\label{eq:the-hyphotesis} \end{equation}&lt;/script&gt;

&lt;p&gt;By substituting \eqref{eq:the-hyphotesis} into \eqref{eq:cost-function}, we obtain&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}  J(\theta) &amp;= \frac{1}{2m} \sum_{i=1}^{m}{(\theta_0 + \theta_1 x_1^{(i)} + \cdots + \theta_n x_n^{(i)} - y^{(i)})^2}  \tag{3}\label{eq:derivation-1} \\
&amp;= \frac{1}{2m} ((\theta_0 + \theta_1 x_1^{(1)} + \cdots + \theta_n x_n^{(1)} - y^{(1)} )^2 + \cdots + (\theta_0 + \theta_1 x_1^{(m)} + \cdots + \theta_n x_n^{(m)} - y^{(m)} )^2 ) \tag{4}\label{eq:derivation-2} \\
&amp;= \frac{1}{2m} \underbrace{ \begin{bmatrix} (\theta_0 + \theta_1 x_1^{(1)} + \cdots + \theta_n x_n^{(1)} - y^{(1)}) &amp; \cdots &amp; (\theta_0 + \theta_1 x_1^{(m)} + \cdots + \theta_n x_n^{(m)} - y^{(m)}) \end{bmatrix}}_{\text{matrix with size: } 1 \times m} \underbrace{\begin{bmatrix} (\theta_0 +\theta_1 x_1^{(1)} + \cdots + \theta_n x_n^{(1)} - y^{(1)}) \\
\vdots \\
(\theta_0 +\theta_1 x_1^{(m)} + \cdots + \theta_n x_n^{(m)} - y^{(1)})
 \end{bmatrix}}_{\text{matrix with size: } m \times 1} \tag{5}\label{eq:derivation-3} \\
 &amp;= \frac{1}{2m} \left( \begin{bmatrix} (\theta_0 + \theta_1 x_1^{(1)} + \cdots + \theta_n x_n^{(1)} - y^{(1)}) &amp; \cdots &amp; (\theta_0 + \theta_1 x_1^{(m)} + \cdots + \theta_n x_n^{(m)} - y^{(m)}) \end{bmatrix} - \begin{bmatrix} y^{(1)} &amp; \cdots &amp; y^{(m)} \end{bmatrix} \right) \left( \begin{bmatrix} \theta_0 + \theta_1 x_1^{(1)} + \cdots + \theta_n x_n^{(1)} \\
 \theta_0 + \theta_1 x_1^{(2)} + \cdots + \theta_n x_n^{(2)} \\
 \vdots \\
 \theta_0 + \theta_1 x_1^{(m)} + \cdots + \theta_n x_n^{(m)} \end{bmatrix}  - \begin{bmatrix} y^{(1)} \\ 
 y^{(2)} \\
 \vdots \\
 y^{(m)} \end{bmatrix} \right) \tag{6}\label{eq:derivation-4} \\
 &amp;= \frac{1}{2m} \left( \begin{bmatrix} \theta_0 &amp; \theta_1 &amp; \cdots &amp; \theta_n \end{bmatrix} \begin{bmatrix} 1 &amp; 1 &amp; \cdots &amp; 1 \\ 
 x_1^{(1)} &amp; x_1^{(2)} &amp; \cdots &amp; x_1^{(m)} \\
 \vdots &amp; \vdots &amp; \cdots &amp; \vdots \\
 x_n^{(1)} &amp; x_n^{(2)} &amp; \cdots &amp; x_n^{(m)} \\\end{bmatrix} - \begin{bmatrix} y^{(1)} &amp; \cdots &amp; y^{(m)} \end{bmatrix} \right) \left( \begin{bmatrix} 1 &amp; x_1^{(1)} &amp; \cdots &amp; x_n^{(1)} \\
  1 &amp; x_1^{(2)} &amp; \cdots &amp; x_n^{(2)} \\
  \vdots &amp; \vdots &amp; \cdots &amp; \vdots \\
  1 &amp; x_1^{(m)} &amp; \cdots &amp; x_n^{(m)} \end{bmatrix} \begin{bmatrix} \theta_0 \\ 
 \theta_1 \\
 \vdots \\
 \theta_n \end{bmatrix} - \begin{bmatrix} y^{(1)} \\
  y^{(2)} \\ 
  \ldots \\
  y^{(m)} \end{bmatrix} \right) \tag{7}\label{eq:derivation-5}  \end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;By defining&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation} \theta = \begin{bmatrix} \theta_0 \\ 
\theta_1 \\ 
\vdots \\
\theta_n \end{bmatrix} \tag{8}\label{eq:defining-theta} \end{equation}&lt;/script&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation} X = \begin{bmatrix} 1 &amp; x_1^{(1)} &amp; \cdots &amp; x_n^{(1)} \\
1      &amp; x_1^{(2)} &amp; \cdots  &amp; x_n^{(2)} \\ 
\vdots &amp; \vdots    &amp;  \vdots &amp;  \vdots  \\
1 &amp; x_1^{(m)} &amp; \cdots &amp; x_n^{(m)} \end{bmatrix} \tag{9}\label{eq:defining-X} \end{equation} %]]&gt;&lt;/script&gt;

&lt;p&gt;also&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation} Y = \begin{bmatrix} y^{(1)} \\ 
y^{(2)} \\ 
\vdots \\
y^{(m)} \end{bmatrix}, \tag{10}\label{eq:defining-Y} \end{equation}&lt;/script&gt;

&lt;p&gt;equation \eqref{eq:derivation-5} becomes&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align} J(\theta) &amp;= \frac{1}{2m} (\theta^T X^T - Y^T)(X \theta - Y) &amp;&amp;\text{by transpose property} \tag{11}\label{eq:derivation-6} \\
&amp;= \frac{1}{2m} (\theta^T X^T X \theta - \underbrace{\theta^T X^T Y}_{\text{a scalar}} - \underbrace{Y^T X \theta}_{\text{a scalar}} + Y^T Y)  &amp;&amp;\text{by matrix multiplication} \tag{12}\label{eq:derivation-7} \\
&amp;= \frac{1}{2m} (\theta^T X^T X \theta - \theta^T X^T Y - (X \theta)^T Y + Y^T Y) &amp;&amp;\text{by rearranging a scalar} \tag{13}\label{eq:derivation-8} \\
&amp;= \frac{1}{2m} (\theta^T X^T X \theta - \theta^T X^T Y - \theta^T X^T Y + Y^T Y) &amp;&amp;\text{by transpose property} \tag{14}\label{eq:derivation-9} \\
&amp;= \frac{1}{2m} (\underbrace{\theta^T X^T X \theta}_{\text{Part I}} - \underbrace{2 \theta^T X^T Y}_{\text{Part II}} + Y^T Y) &amp;&amp;\text{by summation property} \tag{15}\label{eq:derivation-10} \\
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;We have arrived into a matrix form from &lt;strong&gt;linear regression cost function&lt;/strong&gt;. Our next step would be:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;How can we minimize the &lt;strong&gt;cost function&lt;/strong&gt; in Equation \eqref{eq:derivation-10}?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;We will employ the derivation formula from &lt;a href=&quot;https://en.wikipedia.org/wiki/Matrix_calculus&quot;&gt;Matrix Calculus&lt;/a&gt;; specifically, we use &lt;strong&gt;two scalar-by-vector identities&lt;/strong&gt; with &lt;strong&gt;denominator layout&lt;/strong&gt; (result: column vector). The identities are as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation} 
	\frac{\partial \mathbf{x}^T \mathbf{A} \mathbf{x} }{\partial \mathbf{x}} = 2 \mathbf{A} \mathbf{x} \tag{16}\label{eq:identity-1}
\end{equation}&lt;/script&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation} 
	\frac{\partial \mathbf{a}^T \mathbf{x} }{\partial \mathbf{x}} = \frac{\partial \mathbf{x}^T \mathbf{a} }{\partial \mathbf{x}} = \mathbf{a} \tag{17}\label{eq:identity-2}
\end{equation}&lt;/script&gt;

&lt;p&gt;Now equipped with these identities, let us minimize Equation \eqref{eq:derivation-10} by computing the first derivation of $J(\theta)$; specifically, the Part I is computed with Equation \eqref{eq:identity-1} and Part II with Equation \eqref{eq:identity-2}:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation} 
	\frac{\partial J }{\partial \theta} = \frac{1}{2m} ( 2 X^T X \theta - 2 X^T Y ) \tag{18}\label{eq:derivation-of-J}
\end{equation}&lt;/script&gt;

&lt;p&gt;In order to find $\theta$ which minimize Equation \eqref{eq:derivation-10}, we need to solve&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align} \frac{\partial J}{\partial \theta} = 0 &amp;\Longleftrightarrow \frac{1}{2m} ( 2 X^T X \theta - 2 X^T Y ) = 0 \\
 &amp;\Longleftrightarrow 2 X^T X \theta - 2 X^T Y = 0 \\
   &amp;\Longleftrightarrow 2 X^T X \theta = 2 X^T Y  \\ 
   &amp;\Longleftrightarrow X^T X \theta = X^T Y \\ 
   &amp;\Longleftrightarrow \theta = (X^T X)^{-1} X^T Y &amp;&amp;\text{by inverse matrix} \tag{19}\label{eq:final-result} \end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;At last, we have derived &lt;strong&gt;the normal equation of linear regression model&lt;/strong&gt; that is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation} \bbox[5px,border:2px solid blue] {\theta = (X^T X)^{-1} X^T Y}. \end{equation}&lt;/script&gt;</content><author><name></name></author><category term="Machine-Learning" /><summary type="html">This article is inspired by an excellent post by Eli Bendersky. Letâ€™s continue with the derivation.</summary></entry><entry xml:lang="EN"><title type="html">Building Logistic Regression Model from Linear Regression Model</title><link href="http://localhost:4000/machine-learning/Logistic_Regression/" rel="alternate" type="text/html" title="Building Logistic Regression Model from Linear Regression Model" /><published>2019-08-17T00:00:00+07:00</published><updated>2019-08-17T00:00:00+07:00</updated><id>http://localhost:4000/machine-learning/Logistic_Regression</id><content type="html" xml:base="http://localhost:4000/machine-learning/Logistic_Regression/">&lt;p&gt;A model called &lt;strong&gt;logistic regression&lt;/strong&gt; is introduced in &lt;a href=&quot;https://www.coursera.org/learn/machine-learning/home/week/3&quot;&gt;Week 3&lt;/a&gt; of &lt;a href=&quot;https://www.coursera.org/learn/machine-learning/home/welcome&quot;&gt;&lt;em&gt;Machine Learning&lt;/em&gt; course taught by Andrew Ng&lt;/a&gt;. This post tries to explain how to obtain &lt;strong&gt;logistic regression&lt;/strong&gt; model from &lt;em&gt;linear regression&lt;/em&gt; model which has been explained in &lt;a href=&quot;https://www.coursera.org/learn/machine-learning/home/week/1&quot;&gt;Week 1&lt;/a&gt; and &lt;a href=&quot;https://www.coursera.org/learn/machine-learning/home/week/2&quot;&gt;Week 2&lt;/a&gt; of the course. Therefore, I recommend all readers for studying &lt;a href=&quot;https://www.coursera.org/learn/machine-learning/home/week/1&quot;&gt;Week 1&lt;/a&gt; and &lt;a href=&quot;https://www.coursera.org/learn/machine-learning/home/week/2&quot;&gt;Week 2&lt;/a&gt; before reading this post.&lt;/p&gt;

&lt;p&gt;Week 1 of the course shows that &lt;em&gt;linear regression&lt;/em&gt; model has the capability to solve classification problem with prediction model (&lt;em&gt;hypothesis&lt;/em&gt;) as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation} \bbox[5px,border:2px solid blue] {h_\theta(x) = \theta^T x} \tag{1}\label{eq:linear-regression} \end{equation}&lt;/script&gt;

&lt;p&gt;with &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\theta = \begin{bmatrix} \theta_0 &amp; \theta_1 &amp; \theta_2 &amp; \cdots &amp; \theta_n \end{bmatrix}^T %]]&gt;&lt;/script&gt; is called parameter model and &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
x = \begin{bmatrix} 1 &amp; x_1 &amp; x_2 &amp; \cdots &amp; x_n \end{bmatrix}^T %]]&gt;&lt;/script&gt; is a &lt;em&gt;test example&lt;/em&gt; whose &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; we would like to predict.&lt;/p&gt;

&lt;p&gt;The question arises as follows:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;How can we derive &lt;strong&gt;logistic regression&lt;/strong&gt; model from &lt;em&gt;linear regression&lt;/em&gt; model?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;As we already know that $h_\theta(x)$ in &lt;strong&gt;logistic regression&lt;/strong&gt; has a range of values between $0$ and $1$; on the other hand, $h_\theta(x)$ in &lt;em&gt;linear regression&lt;/em&gt; has a range of values between $-\infty$ and $\infty$. Therefore, $h_\theta(x)$ which belongs to &lt;strong&gt;logistic regression&lt;/strong&gt; needs to be converted into $-\infty &amp;lt; h_\theta(x) &amp;lt; \infty$. Moreover, after the conversion is done, the new $h_\theta(x)$ shall be substituted into Equation \eqref{eq:linear-regression}.&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Let us start by converting $h_\theta(x)$ which belongs to &lt;strong&gt;logistic regression&lt;/strong&gt; and then substitute the new $h_\theta(x)$ into Equation \eqref{eq:linear-regression}.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}  0 &lt; h_\theta(x) &lt; 1 &amp;\Longleftrightarrow  0 &lt; \frac{h_\theta(x)}{1-h_\theta(x)} &lt;  \infty \tag{2}\label{eq:odds-ratio} \\ 
	                    &amp;\Longleftrightarrow  -\infty &lt; \underbrace{\log \left( \frac{h_\theta(x)}{1-h_\theta(x)} \right)}_{\text{the new }h_\theta(x)} &lt; \infty \tag{3}\label{eq:logit}.  \end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;$\frac{h_\theta(x)}{1-h_\theta(x)}$ in Equation \eqref{eq:odds-ratio} is called &lt;strong&gt;odds ratio&lt;/strong&gt; and $\log \left( \frac{h_\theta(x)}{1-h_\theta(x)} \right)$ in Equation \eqref{eq:logit} is named as &lt;strong&gt;logit&lt;/strong&gt; function &lt;a href=&quot;https://www.packtpub.com/big-data-and-business-intelligence/python-machine-learning-second-edition&quot;&gt;(Raschka &amp;amp; Mirjalili, 2017)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Now that the new $h_\theta(x)$ has the range of values between $-\infty &amp;lt; h_\theta(x) &amp;lt; \infty$, this new $ h_\theta(x) $ can be substituted into Equation \eqref{eq:linear-regression} as follows:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align} \log \left( \frac{h_\theta(x)}{1-h_\theta(x)} \right) = \theta^T x &amp;\Longleftrightarrow e^{\log \left( \frac{h_\theta(x)}{1-h_\theta(x)} \right)} = e^{\theta^T x}   \\
	     &amp;\Longleftrightarrow  \frac{h_\theta(x)}{1-h_\theta(x)} = e^{\theta^T x} \\
	     &amp;\Longleftrightarrow  h_\theta(x) = e^{\theta^T x} - h_\theta(x) e^{\theta^T x}  \\
	     &amp;\Longleftrightarrow  h_\theta(x) + h_\theta(x) e^{\theta^T x} = e^{\theta^T x}   \\ 
	     &amp;\Longleftrightarrow  h_\theta(x) \left( 1 + e^{\theta^T x} \right) = e^{\theta^T x} \\
	     &amp;\Longleftrightarrow  h_\theta(x) = \frac{e^{\theta^T x}}{1 + e^{\theta^T x}}  \\
	     &amp;\Longleftrightarrow  h_\theta(x) = \frac{e^{\theta^T x}}{1 + e^{\theta^T x}}  \times \frac{e^{-\theta^T x}}{e^{-\theta^T x}} \\
	     &amp;\Longleftrightarrow h_\theta(x) = \frac{1}{1+e^{-\theta^T x}}.
	\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Accordingly, &lt;strong&gt;logistic regression&lt;/strong&gt; model that we have derived is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation} \bbox[5px,border:2px solid blue] {h_\theta(x) = \frac{1}{1+e^{-\theta^T x}}} \tag{4}\label{eq:sigmoid} \end{equation}&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Logistic regression&lt;/strong&gt; model in Equation \eqref{eq:sigmoid} is popularly also called &lt;strong&gt;sigmoid function&lt;/strong&gt;. Finally, we have successfully derived &lt;strong&gt;logistic regression&lt;/strong&gt; model (Equation \eqref{eq:sigmoid}) from &lt;em&gt;linear regression&lt;/em&gt; model (Equation \eqref{eq:linear-regression}).   &lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;
&lt;p&gt;Raschka, S. and Mirjalili, V. (2017) &lt;em&gt;Python Machine Learning Second Edition&lt;/em&gt;. Packt Publishing Ltd. Page 59-60.&lt;/p&gt;</content><author><name></name></author><category term="Machine-Learning" /><summary type="html">A model called logistic regression is introduced in Week 3 of Machine Learning course taught by Andrew Ng. This post tries to explain how to obtain logistic regression model from linear regression model which has been explained in Week 1 and Week 2 of the course. Therefore, I recommend all readers for studying Week 1 and Week 2 before reading this post.</summary></entry><entry xml:lang="EN"><title type="html">Compute the Gradient of Cost Function from Logistic Regression</title><link href="http://localhost:4000/machine-learning/Gradient_Descent_for_Logistic_Regression/" rel="alternate" type="text/html" title="Compute the Gradient of Cost Function from Logistic Regression" /><published>2019-04-08T00:00:00+07:00</published><updated>2019-04-08T00:00:00+07:00</updated><id>http://localhost:4000/machine-learning/Gradient_Descent_for_Logistic_Regression</id><content type="html" xml:base="http://localhost:4000/machine-learning/Gradient_Descent_for_Logistic_Regression/">&lt;p&gt;In Machine Learning literature, we have a &lt;em&gt;design matrix&lt;/em&gt; ($X$) that represents our dataset and specifically has a shape as follows&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{equation} X = \begin{bmatrix} 1 &amp; x^{(1)} \\
1      &amp; x^{(2)} \\
\vdots &amp; \vdots  \\
1      &amp; x^{(m)}   \end{bmatrix}.  \end{equation} \tag{1}\label{eq:x-dataset} %]]&gt;&lt;/script&gt;

&lt;p&gt;We say that the &lt;em&gt;design matrix&lt;/em&gt; ($X$) in \eqref{eq:x-dataset} has $m$ &lt;em&gt;training examples&lt;/em&gt; and 1 &lt;em&gt;feature&lt;/em&gt;, $x$.&lt;/p&gt;

&lt;p&gt;The &lt;strong&gt;logistic regression&lt;/strong&gt; model shall be trained on the $X$ dataset. For those who are not familiar with logistic regression model can study the model in &lt;a href=&quot;https://www.coursera.org/learn/machine-learning/home/welcome&quot;&gt;&lt;em&gt;Machine Learning Course by Andrew Ng&lt;/em&gt;&lt;/a&gt; in Week 4. The  membahas model &lt;strong&gt;logistic regression&lt;/strong&gt; yang memiliki bentuk&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\require{cancel} \begin{equation} h_{\theta}(x^{(i)}) = \frac{1}{1 + e^{-\theta_0 - \theta_1 x^{(i)}}}. \tag{2}\label{eq:model-logistic}\end{equation}&lt;/script&gt;

&lt;p&gt;Lebih lanjut, model &lt;strong&gt;logistic regression&lt;/strong&gt; memiliki &lt;em&gt;cost function&lt;/em&gt; $J(\theta)$ sebagai berikut:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation} J(\theta) = \frac{1}{m} \sum_{i=1}^{m} \text{Cost}(h_{\theta}(x^{(i)}), y^{(i)}) \tag{3}\label{eq:cost-function} \end{equation}&lt;/script&gt;

&lt;p&gt;dengan&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation} \text{Cost}(h_{\theta}(x^{(i)}), y^{(i)}) = -y^{(i)} \log( h_{\theta}(x^{(i)}) ) - (1-y^{(i)}) \log(1-h_{\theta}(x^{(i)})) \end{equation} \tag{4}\label{eq:cost-logistic}&lt;/script&gt;

&lt;p&gt;dan diketahui juga bahwa $x^{(i)}$ adalah &lt;em&gt;training example&lt;/em&gt; ke-$i$, dan $y^{(i)}$ adalah label atau class dari &lt;em&gt;training example&lt;/em&gt; $x^{(i)}$. &lt;br /&gt;
Tulisan ini hendak menjelaskan bagaimana menghitung turunan parsial dari &lt;em&gt;cost function&lt;/em&gt; model &lt;strong&gt;logistic regression&lt;/strong&gt; terhadap $\theta_0$ dan $\theta_1$. Turunan parsial ini juga sering disebut dengan &lt;em&gt;gradient&lt;/em&gt;, $\frac{\partial J}{\partial \theta}$.  &lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;h4 id=&quot;bentuk-komplit-cost-function-dari-model-logistic-regression&quot;&gt;&lt;strong&gt;Bentuk Komplit &lt;em&gt;Cost Function&lt;/em&gt; dari Model Logistic Regression&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;Dengan menggabungkan Persamaan \eqref{eq:cost-function} dan \eqref{eq:cost-logistic} diperoleh &lt;em&gt;cost function&lt;/em&gt; yang detil sebagai berikut:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align} J(\theta) &amp;= \frac{1}{m} \sum_{i=1}^{m} \text{Cost}(h_{\theta}(x^{(i)}), y^{(i)}) \\
                           &amp;= \frac{1}{m} \sum_{i=1}^{m} \left( -y^{(i)} \log (h_{\theta}(x^{(i)})) - (1-y^{(i)}) \log (1 - h_{\theta}(x^{(i)})) \right) \\
                           &amp;= -\frac{1}{m} \sum_{i=1}^{m} \left( y^{(i)} \log (h_{\theta}(x^{(i)})) + (1-y^{(i)}) \log (1 - h_{\theta}(x^{(i)})) \right). \tag{5}\label{eq:cost-function-complete} \end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Selanjutnya, akan dicari $\frac{\partial J}{\partial \theta_0}$ dan $\frac{\partial J}{\partial \theta_1}$ tetapi sebelumnya, kita perlu menghitung $\frac{\partial h_{\theta}}{ \partial \theta_0 }$ dan $\frac{\partial h_{\theta}}{ \partial \theta_1 }$. &lt;br /&gt;
Sekarang kita akan menghitung turunan partial dari $h_{\theta}(x)$ terhadap $\theta_0$ atau $\frac{\partial h_{\theta}}{ \partial \theta_0 }$. &lt;br /&gt;
Dari pelajaran &lt;strong&gt;Kalkulus&lt;/strong&gt;, diketahui bahwa turunan dari $\frac{u(x)}{v(x)}$  dengan masing-masing $u(x)$ dan $v(x)$ merupakan fungsi dari $x$ adalah&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation} \frac{u^{\prime} v - u v^{\prime} }{ v^2 }. \tag{6}\label{eq:formula-derivatif} \end{equation}&lt;/script&gt;

&lt;p&gt;dengan $u^{\prime}$ adalah turunan pertama fungsi $u$ dan $v^{\prime}$ adalah turunan pertama fungsi $v$. &lt;br /&gt;
Kita akan menggunakan formula turunan di Persamaan \eqref{eq:formula-derivatif} untuk menghitung $\frac{\partial h_{\theta}}{ \partial \theta_0 }$ dan $\frac{\partial h_{\theta}}{ \partial \theta_1 }$ sebagai berikut:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align} \frac{\partial h_{\theta}}{ \partial \theta_0 } &amp;= \frac{0 + e^{-\theta_0 - \theta_1 x}}{(1 + e^{-\theta_0 - \theta_1 x})^2} = \frac{e^{-\theta_0 - \theta_1 x}}{(1 + e^{-\theta_0 - \theta_1 x})^2} \\ 
 &amp;= \left( \frac{1}{1 + e^{-\theta_0 - \theta_1 x}} \right) \left( \frac{e^{-\theta_0 - \theta_1 x}}{1 + e^{-\theta_0 - \theta_1 x}} \right)  \\
 &amp;= \left( \frac{1}{1 + e^{-\theta_0 - \theta_1 x}} \right) \left( 1 - \frac{1}{1 + e^{-\theta_0 - \theta_1 x}} \right) \\
 &amp;= h_\theta(x) (1 - h_\theta(x)) \tag{7}\label{eq:formula-derivatif-theta0}
  \end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;dan&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align} \frac{\partial h_{\theta}}{ \partial \theta_1 } &amp;= \frac{0 + e^{-\theta_0 - \theta_1 x} x}{(1 + e^{-\theta_0 - \theta_1 x})^2} = \frac{e^{-\theta_0 - \theta_1 x} x}{(1 + e^{-\theta_0 - \theta_1 x})^2} \\ 
 &amp;= \left( \frac{1}{1 + e^{-\theta_0 - \theta_1 x}} \right) \left( \frac{e^{-\theta_0 - \theta_1 x}}{1 + e^{-\theta_0 - \theta_1 x}} \right)x  \\
 &amp;= \left( \frac{1}{1 + e^{-\theta_0 - \theta_1 x}} \right) \left( 1 - \frac{1}{1 + e^{-\theta_0 - \theta_1 x}} \right)x \\
 &amp;= h_\theta(x) (1 - h_\theta(x)) x. \tag{8}\label{eq:formula-derivatif-theta1}
  \end{align} %]]&gt;&lt;/script&gt;  &lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;h4 id=&quot;menghitung-fracpartial-jpartial-theta_0&quot;&gt;&lt;strong&gt;Menghitung $\frac{\partial J}{\partial \theta_0}$&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;Turunan parsial $\frac{\partial J}{\partial \theta_0}$ dapat dihitung sbb:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align} \frac{\partial J}{\partial \theta_0} &amp;= \frac{\partial }{\partial \theta_0} \left( -\frac{1}{m} \sum_{i=1}^{m} \left( y^{(i)} \log (h_{\theta}(x^{(i)})) + (1-y^{(i)}) \log (1 - h_{\theta}(x^{(i)})) \right) \right) \\
 &amp;= -\frac{1}{m} \frac{\partial }{\partial \theta_0} \left( \sum_{i=1}^{m} \left( y^{(i)} \log (h_{\theta}(x^{(i)})) + (1-y^{(i)}) \log (1 - h_{\theta}(x^{(i)})) \right) \right) \\
 &amp;= -\frac{1}{m} \sum_{i=1}^{m} \left( y^{(i)} \underbrace{\frac{\partial }{\partial \theta_0} \left( \log (h_{\theta}(x^{(i)})) \right)}_{\text{Bagian I}}  + (1-y^{(i)}) \underbrace{\frac{\partial }{\partial \theta_0} \left(\log (1 - h_{\theta}(x^{(i)})) \right)}_{\text{Bagian II}} \right). \tag{9}\label{eq:bagian2-theta0} \end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Bagian I dari Persamaan \eqref{eq:bagian2-theta0} dihitung dengan teknik aturan rantai (&lt;em&gt;chain rule&lt;/em&gt;) dan hasil Persamaan \eqref{eq:formula-derivatif-theta0} menjadi sebagai berikut:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align} \frac{\partial }{\partial \theta_0} \left( \log (h_{\theta}(x^{(i)})) \right) &amp;= \frac{\partial }{\partial h_{\theta} } \left( \log (h_{\theta}(x^{(i)})) \right) \frac{\partial h_{\theta}}{\partial \theta_0} \\
 &amp;= \frac{1}{h_{\theta}(x^{(i)})} h_\theta(x^{(i)}) (1 - h_\theta(x^{(i)})) \\
 &amp;= \frac{1}{ \cancel{h_{\theta}(x^{(i)})} } \cancel{h_\theta(x^{(i)})} (1 - h_\theta(x^{(i)}))  \\
 &amp;= (1 - h_\theta(x^{(i)})). \tag{10}\label{eq:bagian-I-theta0} \end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Bagian II dari Persamaan \eqref{eq:bagian2-theta0} juga dihitung dengan teknik aturan rantai dan hasil Persamaan \eqref{eq:formula-derivatif-theta0}menjadi sebagai berikut:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align} \frac{\partial }{\partial \theta_0} \left( \log (1 - h_{\theta}(x^{(i)})) \right) &amp;= \frac{\partial }{\partial h_{\theta} } \left( \log ( 1- h_{\theta}(x^{(i)})) \right) \frac{\partial h_{\theta}}{\partial \theta_0} \\
 &amp;= - \frac{1}{1 - h_{\theta}(x^{(i)})} h_\theta(x^{(i)}) (1 - h_\theta(x^{(i)})) \\
 &amp;= - \frac{1}{1 - h_{\theta}(x^{(i)})} (1 - h_\theta(x^{(i)})) h_\theta(x^{(i)})  \\
 &amp;= - \frac{1}{ \cancel{1 - h_{\theta}(x^{(i)})} } \cancel{(1- h_\theta(x^{(i)}))} h_\theta(x^{(i)})  \\
 &amp;= -h_\theta(x^{(i)}). \tag{11}\label{eq:bagian-II-theta0} \end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Dengan mensubstitusi Persamaan \eqref{eq:bagian-I-theta0} dan Persamaan \eqref{eq:bagian-II-theta0} ke Persamaan \eqref{eq:bagian2-theta0} diperoleh&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align} \frac{\partial J}{\partial \theta_0} &amp;= -\frac{1}{m} \sum_{i=1}^{m} \left( y^{(i)} \underbrace{\frac{\partial }{\partial \theta_0} \left( \log (h_{\theta}(x^{(i)})) \right)}_{\text{Bagian I}}  + (1-y^{(i)}) \underbrace{\frac{\partial }{\partial \theta_0} \left(\log (1 - h_{\theta}(x^{(i)})) \right)}_{\text{Bagian II}} \right) \\
&amp;= -\frac{1}{m} \sum_{i=1}^{m} \left( y^{(i)} (1 - h_{\theta}(x^{(i)}))  - (1-y^{(i)}) h_{\theta}(x^{(i)}) \right)  \\
&amp;= -\frac{1}{m} \sum_{i=1}^{m} \left( y^{(i)} - y^{(i)} h_{\theta}(x^{(i)})  - h_{\theta}(x^{(i)}) + y^{(i)} h_{\theta}(x^{(i)}) \right) \\
&amp;= -\frac{1}{m} \sum_{i=1}^{m} \left( y^{(i)} \cancel{- y^{(i)} h_{\theta}(x^{(i)})}  - h_{\theta}(x^{(i)}) \cancel{+ y^{(i)} h_{\theta}(x^{(i)})} \right) \\
&amp;= -\frac{1}{m} \sum_{i=1}^{m} ( y^{(i)} - h_{\theta}(x^{(i)}) )  \\
&amp;= \frac{1}{m} \sum_{i=1}^{m} ( h_{\theta}(x^{(i)}) - y^{(i)} ). \end{align} %]]&gt;&lt;/script&gt;  &lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;h4 id=&quot;menghitung-fracpartial-jpartial-theta_1&quot;&gt;&lt;strong&gt;Menghitung $\frac{\partial J}{\partial \theta_1}$&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;Turunan parsial $\frac{\partial J}{\partial \theta_1}$ dapat dihitung dengan cara yang sama sbb:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align} \frac{\partial J}{\partial \theta_1} &amp;= \frac{\partial }{\partial \theta_1} \left( -\frac{1}{m} \sum_{i=1}^{m} \left( y^{(i)} \log (h_{\theta}(x^{(i)})) + (1-y^{(i)}) \log (1 - h_{\theta}(x^{(i)})) \right) \right) \\
 &amp;= -\frac{1}{m} \frac{\partial }{\partial \theta_1} \left( \sum_{i=1}^{m} \left( y^{(i)} \log (h_{\theta}(x^{(i)})) + (1-y^{(i)}) \log (1 - h_{\theta}(x^{(i)})) \right) \right) \\
 &amp;= -\frac{1}{m} \sum_{i=1}^{m} \left( y^{(i)} \underbrace{\frac{\partial }{\partial \theta_1} \left( \log (h_{\theta}(x^{(i)})) \right)}_{\text{Bagian I}}  + (1-y^{(i)}) \underbrace{\frac{\partial }{\partial \theta_1} \left(\log (1 - h_{\theta}(x^{(i)})) \right)}_{\text{Bagian II}} \right). \tag{12}\label{eq:bagian2-theta1} \end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Bagian I dari Persamaan \eqref{eq:bagian2-theta1} dihitung dengan teknik aturan rantai (&lt;em&gt;chain rule&lt;/em&gt;) dan hasil Persamaan \eqref{eq:formula-derivatif-theta1} menjadi sebagai berikut:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align} \frac{\partial }{\partial \theta_1} \left( \log (h_{\theta}(x^{(i)})) \right) &amp;= \frac{\partial }{\partial h_{\theta} } \left( \log (h_{\theta}(x^{(i)})) \right) \frac{\partial h_{\theta}}{\partial \theta_1} \\
 &amp;= \frac{1}{h_{\theta}(x^{(i)})} h_\theta(x^{(i)}) (1 - h_\theta(x^{(i)})) x^{(i)} \\
 &amp;= \frac{1}{ \cancel{h_{\theta}(x^{(i)})} } \cancel{h_\theta(x^{(i)})} (1 - h_\theta(x^{(i)})) x^{(i)} \\
 &amp;= (1 - h_\theta(x^{(i)})) x^{(i)}. \tag{13}\label{eq:bagian-I-theta1} \end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Bagian II dari Persamaan \eqref{eq:bagian2-theta1} juga dihitung dengan teknik aturan rantai dan hasil Persamaan \eqref{eq:formula-derivatif-theta1}menjadi sebagai berikut:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align} \frac{\partial }{\partial \theta_1} \left( \log (1 - h_{\theta}(x^{(i)})) \right) &amp;= \frac{\partial }{\partial h_{\theta} } \left( \log ( 1- h_{\theta}(x^{(i)})) \right) \frac{\partial h_{\theta}}{\partial \theta_1} \\
 &amp;= - \frac{1}{1 - h_{\theta}(x^{(i)})} h_\theta(x^{(i)}) (1 - h_\theta(x^{(i)})) x^{(i)} \\
 &amp;= - \frac{1}{1 - h_{\theta}(x^{(i)})} (1 - h_\theta(x^{(i)})) h_\theta(x^{(i)}) x^{(i)}  \\
 &amp;= - \frac{1}{ \cancel{1 - h_{\theta}(x^{(i)})} } \cancel{(1- h_\theta(x^{(i)}))} h_\theta(x^{(i)}) x^{(i)}  \\
 &amp;= -h_\theta(x^{(i)}) x^{(i)}. \tag{14}\label{eq:bagian-II-theta1} \end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Kembali dengan mensubstitusi Persamaan \eqref{eq:bagian-I-theta1} dan Persamaan \eqref{eq:bagian-II-theta1} ke Persamaan \eqref{eq:bagian2-theta1} diperoleh&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align} \frac{\partial J}{\partial \theta_1} &amp;= -\frac{1}{m} \sum_{i=1}^{m} \left( y^{(i)} \underbrace{\frac{\partial }{\partial \theta_1} \left( \log (h_{\theta}(x^{(i)})) \right)}_{\text{Bagian I}}  + (1-y^{(i)}) \underbrace{\frac{\partial }{\partial \theta_1} \left(\log (1 - h_{\theta}(x^{(i)})) \right)}_{\text{Bagian II}} \right) \\
&amp;= -\frac{1}{m} \sum_{i=1}^{m} \left( y^{(i)} (1 - h_{\theta}(x^{(i)})) x^{(i)}  - (1-y^{(i)}) h_{\theta}(x^{(i)}) x^{(i)} \right)  \\
&amp;= -\frac{1}{m} \sum_{i=1}^{m} \left( y^{(i)} - y^{(i)} h_{\theta}(x^{(i)})  - h_{\theta}(x^{(i)}) + y^{(i)} h_{\theta}(x^{(i)}) \right) x^{(i)} \\
&amp;= -\frac{1}{m} \sum_{i=1}^{m} \left( y^{(i)} \cancel{- y^{(i)} h_{\theta}(x^{(i)})}  - h_{\theta}(x^{(i)}) \cancel{+ y^{(i)} h_{\theta}(x^{(i)})} \right) x^{(i)} \\
&amp;= -\frac{1}{m} \sum_{i=1}^{m} ( y^{(i)} - h_{\theta}(x^{(i)}) ) x^{(i)}   \\
&amp;= \frac{1}{m} \sum_{i=1}^{m} ( h_{\theta}(x^{(i)}) - y^{(i)} ) x^{(i)}.  \end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Jadi &lt;em&gt;gradient&lt;/em&gt; untuk model &lt;strong&gt;logistic regression&lt;/strong&gt; dengan 1 variabel, $x$ dan 2 parameter, $\theta_0$ dan $\theta_1$ adalah&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation} \frac{\partial J}{\partial \theta_0} = \frac{1}{m} \sum_{i=1}^{m} ( h_{\theta}(x^{(i)}) - y^{(i)} )  \end{equation}&lt;/script&gt;

&lt;p&gt;dan&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{equation} \frac{\partial J}{\partial \theta_1} = \frac{1}{m} \sum_{i=1}^{m} ( h_{\theta}(x^{(i)}) - y^{(i)} ) x^{(i)} . \end{equation}&lt;/script&gt;

&lt;p&gt;Secara umum, &lt;em&gt;gradient&lt;/em&gt; untuk model &lt;strong&gt;logistic regression&lt;/strong&gt; dengan $n$ variabel, $x_1, x_2, \ldots, x_n$ dan $n+1$ parameter, $\theta_0, \theta_1, \ldots, \theta_n$ adalah&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;&lt;/script&gt; \begin{equation} \frac{\partial J}{\partial \theta_j} = \frac{1}{m} \sum_{i=1}^{m} ( h_{\theta}(x^{(i)}) - y^{(i)} ) x_{j}^{(i)}  \end{equation} &lt;script type=&quot;math/tex&quot;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;dengan ketika $j=0$, $x_0^{(i)} = 1$ untuk $i = 1, 2, \ldots, m$.&lt;/p&gt;</content><author><name></name></author><category term="Machine-Learning" /><summary type="html">In Machine Learning literature, we have a design matrix ($X$) that represents our dataset and specifically has a shape as follows</summary></entry><entry xml:lang="ID"><title type="html">Gereja Menghasilkan Murid Kristus</title><link href="http://localhost:4000/sermons/Murid_Kristus/" rel="alternate" type="text/html" title="Gereja Menghasilkan Murid Kristus" /><published>2019-03-31T00:00:00+07:00</published><updated>2019-03-31T00:00:00+07:00</updated><id>http://localhost:4000/sermons/Murid_Kristus</id><content type="html" xml:base="http://localhost:4000/sermons/Murid_Kristus/">&lt;p&gt;Khotbah ini merupakan ringkasan khotbah yang dibawakan oleh Pnt. Kurdijanto Harsono pada tanggal 31 April 2019 di KU 2.&lt;/p&gt;

&lt;p&gt;Apabila pabrik sepatu menghasilkan sepatu dan pabrik sepeda menghasilkan sepeda, apakah yang dihasilkan oleh gereja? Gereja menghasilkan &lt;strong&gt;murid Kristus&lt;/strong&gt; melalui proses intensional untuk berjalan bersama Yesus Kristus. Selanjutnya, pembahasan akan membahas &lt;strong&gt;murid Kristus&lt;/strong&gt;, &lt;strong&gt;melalui proses intensional&lt;/strong&gt;, dan &lt;strong&gt;untuk berjalan bersama Yesus Kristus&lt;/strong&gt;.  &lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;h4 id=&quot;murid-kristus&quot;&gt;&lt;strong&gt;Murid Kristus&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;Khotbah didasarkan dari kitab Matius 28:19-20 yang berbunyi:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;Karena itu &lt;strong&gt;pergilah&lt;/strong&gt;, &lt;strong&gt;jadikanlah&lt;/strong&gt; semua bangsa murid-Ku dan &lt;strong&gt;baptislah&lt;/strong&gt; mereka dalam nama Bapa dan Anak dan Roh Kudus, dan &lt;strong&gt;ajarlah&lt;/strong&gt; mereka melakukan segala sesuatu yang telah Kuperintahkan kepadamu. Dan ketahuilah, Aku menyertai kamu senantiasa sampai kepada akhir zaman&lt;/em&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Terdapat 4 kata kerja di 2 ayat tersebut, yaitu:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;pergilah,&lt;/li&gt;
  &lt;li&gt;jadikanlah,&lt;/li&gt;
  &lt;li&gt;baptislah, dan&lt;/li&gt;
  &lt;li&gt;ajarlah.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Dari 4 kata kerja tersebut, yang terutama adalah &lt;strong&gt;menjadikan murid&lt;/strong&gt; (bahasa asli: &lt;em&gt;matheteuo&lt;/em&gt;). Perlu ditekankan bahwa &lt;strong&gt;pemuridan&lt;/strong&gt; bukan hanya berfokus ke dalam tetapi juga berfokus ke luar; contohnya, bagaimana orang lain dapat melihat kehidupan kita.&lt;/p&gt;

&lt;p&gt;Murid Kristus berarti bukan hanya &lt;em&gt;melakukan&lt;/em&gt; apa yang diperintahkan Tuhan tetapi &lt;em&gt;menjadi&lt;/em&gt; seperti Yesus. Konsep ini mengatakan bahwa bukan &lt;em&gt;doing&lt;/em&gt; melainkan &lt;em&gt;being&lt;/em&gt;.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Melakukan &lt;strong&gt;belum tentu&lt;/strong&gt; Menjadi tetapi Menjadi &lt;strong&gt;pasti&lt;/strong&gt; Melakukan.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Contoh &lt;strong&gt;yang Melakukan tetapi tidak Menjadi&lt;/strong&gt; adalah &lt;em&gt;orang muda yang kaya&lt;/em&gt; (Markus 10:17-25, Lukas 18:18-29) dan contoh &lt;strong&gt;yang Menjadi dan Melakukan&lt;/strong&gt; adalah Zakeus (Lukas 19:1-10). &lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;
&lt;h4 id=&quot;melalui-proses-intensional&quot;&gt;&lt;strong&gt;Melalui proses intensional&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;1 Korintus 3:6&lt;/strong&gt; (NLT) menyatakan bahwa&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;My job was to plant the seed in your hearts, and Apollos watered it, but it was God, not we, who made it grow&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Ayat ini berkata bahwa dalam pertumbuhan iman, &lt;strong&gt;perlu juga usaha &amp;amp; kerinduan&lt;/strong&gt; untuk bertumbuh. Menjadi &lt;strong&gt;murid Kristus&lt;/strong&gt; merupakan proses kontinyu dan berulang. Kita menerima Firman Tuhan dan mencoba menerapkanNya dalam kehidupan kita dan pada akhirnya multiplikasi akan terjadi. &lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;h4 id=&quot;untuk-berjalan-bersama-yesus-kristus&quot;&gt;&lt;strong&gt;Untuk berjalan bersama Yesus Kristus&lt;/strong&gt;&lt;/h4&gt;
&lt;p&gt;Matius 28:20b berkata:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;Dan ketahuilah, Aku menyertai kamu senantiasa sampai kepada akhir zaman&lt;/em&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Dalam versi NKJV:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;I am with you always&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Tuhan bersama dengan kita senantiasa. Tuhan ada di depan kita dan memimpin kita.&lt;/p&gt;</content><author><name></name></author><category term="Sermons" /><summary type="html">Khotbah ini merupakan ringkasan khotbah yang dibawakan oleh Pnt. Kurdijanto Harsono pada tanggal 31 April 2019 di KU 2.</summary></entry><entry xml:lang="ID"><title type="html">Contoh Pembuktian Ruang Vektor dengan sangat Detil</title><link href="http://localhost:4000/linear-algebra/Ruang_Vektor/" rel="alternate" type="text/html" title="Contoh Pembuktian Ruang Vektor dengan sangat Detil" /><published>2019-03-22T00:00:00+07:00</published><updated>2019-03-22T00:00:00+07:00</updated><id>http://localhost:4000/linear-algebra/Ruang_Vektor</id><content type="html" xml:base="http://localhost:4000/linear-algebra/Ruang_Vektor/">&lt;p&gt;Tunjukkan bahwa himpunan dari semua matriks berukuran $2 \times 3$ beserta operasi &lt;em&gt;matrix addition&lt;/em&gt; dan &lt;em&gt;scalar multiplication&lt;/em&gt; merupakan sebuah &lt;strong&gt;ruang vektor&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Bukti&lt;/strong&gt;: &lt;br /&gt;
Misalkan himpunan dari semua matriks berukuran $2 \times 3$ beserta operasi &lt;em&gt;matrix addition&lt;/em&gt; dan &lt;em&gt;scalar multiplication&lt;/em&gt; adalah $V$.&lt;/p&gt;

&lt;p&gt;Diketahui juga $A$, $B$, dan $C$ adalah matriks berukuran $2 \times 3$ dan $c$, $d$ adalah skalar dengan spesifikasi sebagai berikut:&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
A = \begin{bmatrix}a_{11} &amp; a_{12} &amp; a_{13}\\a_{21} &amp; a_{22} &amp; a_{23}\end{bmatrix} %]]&gt;&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
B = \begin{bmatrix} 	b_{11} &amp; b_{12} &amp; b_{13} \\ b_{21} &amp; b_{22} &amp; b_{23}\end{bmatrix} %]]&gt;&lt;/script&gt;, dan &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
C = \begin{bmatrix} 	c_{11} &amp; c_{12} &amp; c_{13} \\ c_{21} &amp; c_{22} &amp; c_{23} \end{bmatrix} %]]&gt;&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.amazon.com/Elementary-Linear-Algebra-Ron-Larson/dp/1305658000/&quot;&gt;Larson&lt;/a&gt; (2016) subbab 4.2 hlm. 161 menyatakan bahwa pembuktian suatu himpunan merupakan ruang vektor harus memenuhi 10 aksioma. Berikut akan dibuktikan untuk 10 aksioma tersebut.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Apakah $A+B$ juga ada di dalam $V$?  &lt;br /&gt;
Ya karena   &lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align}
 A + B &amp;= \begin{bmatrix} a_{11} &amp; a_{12} &amp; a_{13} \\ a_{21} &amp; a_{22} &amp; a_{23} \end{bmatrix} + \begin{bmatrix} 	b_{11} &amp; b_{12} &amp; b_{13} \\ b_{21} &amp; b_{22} &amp; b_{23} \end{bmatrix}  \\
       &amp;= \begin{bmatrix} 	a_{11}+b_{11} &amp; a_{12}+b_{12} &amp; a_{13}+b_{13} \\ a_{21}+b_{21} &amp; a_{22}+b_{22} &amp; a_{23}+b_{23}	 \end{bmatrix}.
\end{align} %]]&gt;&lt;/script&gt;  &lt;br /&gt;
&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{bmatrix} a_{11}+b_{11} &amp; a_{12}+b_{12} &amp; a_{13}+b_{13} \\
 a_{21}+b_{21} &amp; a_{22}+b_{22} &amp; a_{23}+b_{23} \end{bmatrix} %]]&gt;&lt;/script&gt; merupakan matriks berukuran $2 \times 3 $, berarti $A+B$ juga ada di dalam $V$.  &lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;Apakah $A+B = B+A$?  &lt;br /&gt;
Ya karena  &lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align} A+B &amp;= \begin{bmatrix} 	a_{11} &amp; a_{12} &amp; a_{13} \\
 a_{21} &amp; a_{22} &amp; a_{23} \end{bmatrix} +  \begin{bmatrix} 	b_{11} &amp; b_{12} &amp; b_{13} \\
 b_{21} &amp; b_{22} &amp; b_{23} \end{bmatrix} \\
                 &amp;= \begin{bmatrix}  a_{11}+b_{11} &amp; a_{12}+b_{12} &amp; a_{13}+b_{13} \\
 a_{21}+b_{21} &amp; a_{22}+b_{22} &amp; a_{23}+b_{23}	
 \end{bmatrix} \\
                 &amp;= \begin{bmatrix} 	b_{11}+a_{11} &amp; b_{12}+a_{12} &amp; b_{13}+a_{13} \\
 b_{21}+a_{21} &amp; b_{22}+a_{22} &amp; b_{23}+a_{23} \end{bmatrix} \\
                 &amp;= \begin{bmatrix} 	b_{11} &amp; b_{12} &amp; b_{13} \\
 b_{21} &amp; b_{22} &amp; b_{23} \end{bmatrix} + \begin{bmatrix} 	a_{11} &amp; a_{12} &amp; a_{13} \\
 a_{21} &amp; a_{22} &amp; a_{23} \end{bmatrix} \\
                 &amp;= B + A.
\end{align} %]]&gt;&lt;/script&gt;      &lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;Apakah $A + (B + C) = (A+B) + C$?  &lt;br /&gt;
Ya karena  &lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align} A+(B+C) &amp;= \begin{bmatrix} a_{11} &amp; a_{12} &amp; a_{13} \\
 a_{21} &amp; a_{22} &amp; a_{23} \end{bmatrix} + \left( \begin{bmatrix} b_{11} &amp; b_{12} &amp; b_{13} \\
 b_{21} &amp; b_{22} &amp; b_{23} \end{bmatrix} + \begin{bmatrix} c_{11} &amp; c_{12} &amp; c_{13} \\
 c_{21} &amp; c_{22} &amp; c_{23} \end{bmatrix} \right) \\
                      &amp;= \begin{bmatrix} a_{11} &amp; a_{12} &amp; a_{13} \\
 a_{21} &amp; a_{22} &amp; a_{23} \end{bmatrix} + \left( \begin{bmatrix} b_{11} + c_{11} &amp; b_{12} + c_{12} &amp; b_{13} + c_{13} \\
 b_{21} + c_{21} &amp; b_{22} + c_{22} &amp; b_{23} + c_{23}	\end{bmatrix} \right) \\
                      &amp;= \begin{bmatrix} a_{11} + b_{11} + c_{11}  &amp; a_{12} + b_{12} + c_{12} &amp; a_{13} + b_{13} + c_{13} \\
 a_{21} + b_{21} + c_{21}  &amp; a_{22} + b_{22} + c_{22} &amp; a_{23} + b_{23} + c_{23} \end{bmatrix} \\
                      &amp;= \begin{bmatrix} a_{11} + b_{11} &amp; a_{12} + b_{12} &amp; a_{13} + b_{13} \\
 a_{21} + b_{21} &amp; a_{22} + b_{22} &amp; a_{23} + b_{23} \end{bmatrix} + \begin{bmatrix} 	 c_{11} &amp;  c_{12} &amp;  c_{13} \\
 c_{21} &amp; c_{22} &amp; c_{23} \end{bmatrix} \\
                      &amp;= \left( \begin{bmatrix} 	a_{11} &amp; a_{12} &amp; a_{13} \\
 a_{21} &amp; a_{22} &amp; a_{23} \end{bmatrix} + \begin{bmatrix} 	b_{11} &amp; b_{12} &amp; b_{13} \\
 b_{21} &amp; b_{22} &amp; b_{23}	  \end{bmatrix} \right) + \begin{bmatrix} 	c_{11} &amp; c_{12} &amp; c_{13} \\
 c_{21} &amp; c_{22} &amp; c_{23}	  \end{bmatrix} \\
                      &amp;= (A+B)+C.
  \end{align} %]]&gt;&lt;/script&gt;   &lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;Apakah $V$ mempunyai &lt;strong&gt;matriks nol&lt;/strong&gt; $\textbf{0}$ sedemikian sehingga untuk setiap matriks $A$ di $V$, $A + \textbf{0} = A$?  &lt;br /&gt;
Ada, matriks $\mathbf{0}$ tersebut adalah  &lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{bmatrix} 0 &amp; 0 &amp; 0 \\
     0 &amp; 0 &amp; 0 \end{bmatrix} %]]&gt;&lt;/script&gt; karena
     &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
A + \mathbf{0} = \begin{bmatrix} a_{11} &amp; a_{12} &amp; a_{13} \\
 a_{21} &amp; a_{22} &amp; a_{23} \end{bmatrix} + \begin{bmatrix} 0 &amp; 0 &amp; 0 \\
     0 &amp; 0 &amp; 0 \end{bmatrix} = \begin{bmatrix} 	a_{11} &amp; a_{12} &amp; a_{13} \\
 a_{21} &amp; a_{22} &amp; a_{23} \end{bmatrix} = A. %]]&gt;&lt;/script&gt;  &lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;Untuk setiap matriks $A$ di dalam $V$, apakah $V$ mempunyai matriks yang dilambangkan dengan $-A$ sedemikian sehingga $A + (-A) = \mathbf{0}$? &lt;br /&gt;
Ada, matriks $-A$ tersebut adalah 
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{bmatrix} -a_{11} &amp; -a_{12} &amp; -a_{13} \\
 -a_{21} &amp; -a_{22} &amp; -a_{23} \end{bmatrix} %]]&gt;&lt;/script&gt;	karena
 &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align} A + (-A) &amp;= \begin{bmatrix} 	a_{11} &amp; a_{12} &amp; a_{13} \\
 a_{21} &amp; a_{22} &amp; a_{23} \end{bmatrix} + \begin{bmatrix} 	-a_{11} &amp; -a_{12} &amp; -a_{13} \\
 -a_{21} &amp; -a_{22} &amp; -a_{23} \end{bmatrix} \\
                           &amp;= \begin{bmatrix} 	a_{11}-a_{11} &amp; a_{12}-a_{12} &amp; a_{13}-a_{13} \\
 a_{21}-a_{21} &amp; a_{22}-a_{22} &amp; a_{23}-a_{23} \end{bmatrix} \\
                           &amp;= \begin{bmatrix} 		0 &amp; 0 &amp; 0 \\
     0 &amp; 0 &amp; 0 \end{bmatrix} \\
                           &amp;= \mathbf{0}.
 \end{align} %]]&gt;&lt;/script&gt;	&lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;Apakah $cA$ juga ada di dalam $V$?  &lt;br /&gt;
Ya, karena
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
cA = c \begin{bmatrix} 	a_{11} &amp; a_{12} &amp; a_{13} \\
 a_{21} &amp; a_{22} &amp; a_{23}
 \end{bmatrix} = \begin{bmatrix} 	ca_{11} &amp; ca_{12} &amp; ca_{13} \\
 ca_{21} &amp; ca_{22} &amp; ca_{23} \end{bmatrix}. %]]&gt;&lt;/script&gt;  &lt;br /&gt;
&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{bmatrix} 	ca_{11} &amp; ca_{12} &amp; ca_{13} \\
 ca_{21} &amp; ca_{22} &amp; ca_{23} \end{bmatrix} %]]&gt;&lt;/script&gt; adalah matriks berukuran $2 \times 3$ sehingga $cA$ juga berada di dalam $V$. &lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;Apakah $c(A+B) = cA + cB$?  &lt;br /&gt;
Ya karena &lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align} c(A+B) &amp;= c \left( \begin{bmatrix} 	a_{11} &amp; a_{12} &amp; a_{13} \\
 a_{21} &amp; a_{22} &amp; a_{23} \end{bmatrix} + \begin{bmatrix} 	b_{11} &amp; b_{12} &amp; b_{13} \\
 b_{21} &amp; b_{22} &amp; b_{23} \end{bmatrix} \right) \\
                     &amp;= c \left( \begin{bmatrix} 	a_{11} + b_{11} &amp; a_{12} + b_{12} &amp; a_{13} + b_{13} \\
 a_{21} + b_{21} &amp; a_{22} + b_{22} &amp; a_{23} + b_{23} \end{bmatrix} \right) \\
                     &amp;= \begin{bmatrix} 	c(a_{11} + b_{11}) &amp; c(a_{12} + b_{12}) &amp; c(a_{13} + b_{13}) \\
 c(a_{21} + b_{21}) &amp; c(a_{22} + b_{22}) &amp; c(a_{23} + b_{23}) \end{bmatrix} \\
                     &amp;= \begin{bmatrix} 	ca_{11} + cb_{11} &amp; ca_{12} + cb_{12} &amp; ca_{13} + cb_{13} \\
 ca_{21} + cb_{21} &amp; ca_{22} + cb_{22} &amp; ca_{23} + cb_{23} \end{bmatrix} \\
                     &amp;= \begin{bmatrix} 	ca_{11} &amp; ca_{12} &amp; ca_{13}  \\
 ca_{21} &amp; ca_{22} &amp; ca_{23}  \end{bmatrix} + \begin{bmatrix} 	cb_{11} &amp; cb_{12} &amp; cb_{13} \\
 cb_{21} &amp; cb_{22} &amp; cb_{23} \end{bmatrix} \\
                     &amp;= c \begin{bmatrix} 	a_{11} &amp; a_{12} &amp; a_{13}  \\
 a_{21} &amp; a_{22} &amp; a_{23}  \end{bmatrix} + c \begin{bmatrix} 	b_{11} &amp; b_{12} &amp; b_{13}  \\
 b_{21} &amp; b_{22} &amp; b_{23}  \end{bmatrix} \\
                     &amp;= cA + cB.
\end{align} %]]&gt;&lt;/script&gt;  &lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;Apakah $(c+d)A = cA + dA$?  &lt;br /&gt;
Ya karena  &lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align} (c+d) A &amp;= (c+d) \begin{bmatrix} 	a_{11} &amp; a_{12} &amp; a_{13}  \\
 a_{21} &amp; a_{22} &amp; a_{23} \\ 		
\end{bmatrix}  \\
                      &amp;= \begin{bmatrix} (c+d) a_{11} &amp; (c+d) a_{12} &amp; (c+d) a_{13}  \\
 (c+d) a_{21} &amp; (c+d) a_{22} &amp; (c+d) a_{23} 	 \end{bmatrix} \\
                      &amp;= \begin{bmatrix} 	c a_{11} +d a_{11} &amp; c a_{12} + d a_{12} &amp; c a_{13} +d a_{13}  \\
 c a_{21} +d a_{21} &amp; c a_{22} + d a_{22} &amp; c a_{23} +d a_{23} 	\end{bmatrix} \\
                      &amp;= \begin{bmatrix} 	c a_{11} &amp; c a_{12}  &amp; c a_{13}   \\
 c a_{21} &amp; c a_{22}  &amp; c a_{23}  \end{bmatrix} + \begin{bmatrix} 	d a_{11} &amp; d a_{12} &amp; d a_{13}  \\
 d a_{21} &amp; d a_{22} &amp; d a_{23} \end{bmatrix} \\
                      &amp;= c \begin{bmatrix} 	a_{11} &amp; a_{12}  &amp; a_{13}   \\
 a_{21} &amp; a_{22}  &amp; a_{23} \end{bmatrix} + d \begin{bmatrix} 	a_{11} &amp; a_{12} &amp; a_{13}  \\
 a_{21} &amp; a_{22} &amp; a_{23} 	\end{bmatrix} \\
                      &amp;= cA + dA.
\end{align} %]]&gt;&lt;/script&gt;  &lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;Apakah $c(dA) = (cd)A$? &lt;br /&gt;
Ya karena &lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align} c(dA) &amp;= c \left( d \begin{bmatrix} 	a_{11} &amp; a_{12} &amp; a_{13}  \\
 a_{21} &amp; a_{22} &amp; a_{23}  \end{bmatrix} \right)  \\
                    &amp;= c \left( \begin{bmatrix} 	da_{11} &amp; da_{12} &amp; da_{13}  \\
 da_{21} &amp; da_{22} &amp; da_{23} \end{bmatrix} \right)  \\
                    &amp;= c \begin{bmatrix} 	da_{11} &amp; da_{12} &amp; da_{13}  \\
 da_{21} &amp; da_{22} &amp; da_{23} \end{bmatrix}  \\
                    &amp;= \begin{bmatrix} 	cda_{11} &amp; cda_{12} &amp; cda_{13}  \\
 cda_{21} &amp; cda_{22} &amp; cda_{23} \end{bmatrix} \\
                    &amp;= \begin{bmatrix} 	(cd)a_{11} &amp; (cd)a_{12} &amp; (cd)a_{13}  \\
 (cd)a_{21} &amp; (cd)a_{22} &amp; (cd)a_{23} \end{bmatrix} \\
                    &amp;= (cd) \begin{bmatrix} 	a_{11} &amp; a_{12} &amp; a_{13}  \\
 a_{21} &amp; a_{22} &amp; a_{23} \end{bmatrix} \\
                    &amp;= (cd) A. 
\end{align} %]]&gt;&lt;/script&gt; &lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;Apakah $1(A) = A$? &lt;br /&gt;
Ya karena  &lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{align} 1(A) &amp;= 1 \begin{bmatrix} 	a_{11} &amp; a_{12} &amp; a_{13}  \\
a_{21} &amp; a_{22} &amp; a_{23} \end{bmatrix} \\
                  &amp;= \begin{bmatrix} 	1a_{11} &amp; 1a_{12} &amp; 1a_{13}  \\
1a_{21} &amp; 1a_{22} &amp; 1a_{23} \end{bmatrix} \\
                  &amp;= \begin{bmatrix} 	a_{11} &amp; a_{12} &amp; a_{13}  \\
a_{21} &amp; a_{22} &amp; a_{23} \end{bmatrix}  \\
                  &amp;= A.
\end{align} %]]&gt;&lt;/script&gt;  &lt;br /&gt;
&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Karena $V$ memenuhi 10 aksioma dari definisi suatu ruang vektor, $V$ atau himpunan dari semua matriks berukuran $2 \times 3$ beserta operasi &lt;em&gt;matrix addition&lt;/em&gt; dan &lt;em&gt;scalar multiplication&lt;/em&gt; merupakan sebuah &lt;strong&gt;ruang vektor&lt;/strong&gt;.   $\square$&lt;/p&gt;</content><author><name></name></author><category term="Linear-Algebra" /><summary type="html">Tunjukkan bahwa himpunan dari semua matriks berukuran $2 \times 3$ beserta operasi matrix addition dan scalar multiplication merupakan sebuah ruang vektor. Bukti: Misalkan himpunan dari semua matriks berukuran $2 \times 3$ beserta operasi matrix addition dan scalar multiplication adalah $V$.</summary></entry></feed>